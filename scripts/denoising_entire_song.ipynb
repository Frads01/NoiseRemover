{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c7ea426",
   "metadata": {
    "id": "OYfLqR29sPnz",
    "papermill": {
     "duration": 0.006192,
     "end_time": "2025-08-22T14:54:23.573472",
     "exception": false,
     "start_time": "2025-08-22T14:54:23.567280",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Denoising intera canzone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956b2fa5",
   "metadata": {
    "id": "RisdxjMgEB7i",
    "papermill": {
     "duration": 0.00477,
     "end_time": "2025-08-22T14:54:23.583643",
     "exception": false,
     "start_time": "2025-08-22T14:54:23.578873",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import librerie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2255170",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T14:54:23.594419Z",
     "iopub.status.busy": "2025-08-22T14:54:23.594154Z",
     "iopub.status.idle": "2025-08-22T14:54:27.979469Z",
     "shell.execute_reply": "2025-08-22T14:54:27.978595Z"
    },
    "id": "yycpjxF9EEkE",
    "papermill": {
     "duration": 4.392405,
     "end_time": "2025-08-22T14:54:27.980999",
     "exception": false,
     "start_time": "2025-08-22T14:54:23.588594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib\n",
    "matplotlib.use('Agg') # backend per salvataggio file, no GUI \n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff() # Disabilita modalità interattiva     \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8756fa7d",
   "metadata": {
    "id": "46TSDkqFEuKv",
    "papermill": {
     "duration": 0.004616,
     "end_time": "2025-08-22T14:54:28.016722",
     "exception": false,
     "start_time": "2025-08-22T14:54:28.012106",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Parametri per la trasformazione STFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d92993ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T14:54:28.027646Z",
     "iopub.status.busy": "2025-08-22T14:54:28.027436Z",
     "iopub.status.idle": "2025-08-22T14:54:28.224076Z",
     "shell.execute_reply": "2025-08-22T14:54:28.223342Z"
    },
    "id": "BdHJ8JPvEzOg",
    "papermill": {
     "duration": 0.203361,
     "end_time": "2025-08-22T14:54:28.225401",
     "exception": false,
     "start_time": "2025-08-22T14:54:28.022040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 44100\n",
    "n_fft = 2048 # grandezza della finestra (risoluzione in frequenza) - nel paper è 3072, ottima per il parlato\n",
    "hop_length = 512 # salto tra una finestra e l’altra (risoluzione temporale) - nel paper è 768\n",
    "\n",
    "window = torch.hann_window(n_fft).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacb66ed",
   "metadata": {
    "id": "SHS5BTBuBTtz",
    "papermill": {
     "duration": 0.004951,
     "end_time": "2025-08-22T14:54:28.262887",
     "exception": false,
     "start_time": "2025-08-22T14:54:28.257936",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Definizione dei diversi layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce09cf3",
   "metadata": {
    "id": "WWvdTgRc9NKh",
    "papermill": {
     "duration": 0.004662,
     "end_time": "2025-08-22T14:54:28.272465",
     "exception": false,
     "start_time": "2025-08-22T14:54:28.267803",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Layer convoluzionale per segnali complessi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c76646f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T14:54:28.283119Z",
     "iopub.status.busy": "2025-08-22T14:54:28.282910Z",
     "iopub.status.idle": "2025-08-22T14:54:28.288719Z",
     "shell.execute_reply": "2025-08-22T14:54:28.288177Z"
    },
    "id": "2QgFF95s9XM1",
    "papermill": {
     "duration": 0.012474,
     "end_time": "2025-08-22T14:54:28.289749",
     "exception": false,
     "start_time": "2025-08-22T14:54:28.277275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ComplexConv2d(nn.Module):  # convoluzione 2D su numeri complessi\n",
    "  def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "      super().__init__()\n",
    "\n",
    "      self.in_channels = in_channels\n",
    "      self.out_channels = out_channels\n",
    "      self.kernel_size = kernel_size\n",
    "      self.padding = padding\n",
    "      self.stride = stride\n",
    "\n",
    "      # crea una convoluzione per la parte reale:\n",
    "      self.real_conv = nn.Conv2d(in_channels=self.in_channels,\n",
    "                                 out_channels=self.out_channels,\n",
    "                                 kernel_size=self.kernel_size,\n",
    "                                 padding=self.padding,\n",
    "                                 stride=self.stride)\n",
    "\n",
    "      # crea un’altra convoluzione per la parte immaginaria.\n",
    "      # Nota: è separata, quindi ha i suoi pesi e bias distinti.\n",
    "      self.im_conv = nn.Conv2d(in_channels=self.in_channels,\n",
    "                               out_channels=self.out_channels,\n",
    "                               kernel_size=self.kernel_size,\n",
    "                               padding=self.padding,\n",
    "                               stride=self.stride)\n",
    "\n",
    "      # Glorot initialization.\n",
    "      nn.init.xavier_normal_(self.real_conv.weight)\n",
    "      nn.init.xavier_normal_(self.im_conv.weight)\n",
    "\n",
    "  def forward(self, x):  # x: è un tensore che contiene, sull’ultima dimensione, la parte reale e immaginaria\n",
    "        x_real = x[..., 0]\n",
    "        x_im = x[..., 1]\n",
    "\n",
    "        # calcolo convoluzione complessa\n",
    "        c_real = self.real_conv(x_real) - self.im_conv(x_im)\n",
    "        c_im = self.im_conv(x_real) + self.real_conv(x_im)\n",
    "\n",
    "        # combino le due parti (reale e immaginaria) di nuovo insieme, lungo l’ultima dimensione (dim = -1), per restituire un tensore complesso.\n",
    "        output = torch.stack([c_real, c_im], dim=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e87114",
   "metadata": {
    "id": "sPG_oG9ZAytW",
    "papermill": {
     "duration": 0.005081,
     "end_time": "2025-08-22T14:54:28.299709",
     "exception": false,
     "start_time": "2025-08-22T14:54:28.294628",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Layer per deconvoluzione di segnali complessi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f91fce9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T14:54:28.310311Z",
     "iopub.status.busy": "2025-08-22T14:54:28.310104Z",
     "iopub.status.idle": "2025-08-22T14:54:28.315859Z",
     "shell.execute_reply": "2025-08-22T14:54:28.315343Z"
    },
    "id": "hwUoRLXXA5Op",
    "papermill": {
     "duration": 0.012084,
     "end_time": "2025-08-22T14:54:28.316853",
     "exception": false,
     "start_time": "2025-08-22T14:54:28.304769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ComplexConvTranspose2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, output_padding=0, padding=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.output_padding = output_padding\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "\n",
    "        self.real_convt = nn.ConvTranspose2d(in_channels=self.in_channels,\n",
    "                                            out_channels=self.out_channels,\n",
    "                                            kernel_size=self.kernel_size,\n",
    "                                            output_padding=self.output_padding,\n",
    "                                            padding=self.padding,\n",
    "                                            stride=self.stride)\n",
    "\n",
    "        self.im_convt = nn.ConvTranspose2d(in_channels=self.in_channels,\n",
    "                                            out_channels=self.out_channels,\n",
    "                                            kernel_size=self.kernel_size,\n",
    "                                            output_padding=self.output_padding,\n",
    "                                            padding=self.padding,\n",
    "                                            stride=self.stride)\n",
    "\n",
    "\n",
    "        # Glorot initialization.\n",
    "        nn.init.xavier_normal_(self.real_convt.weight)\n",
    "        nn.init.xavier_normal_(self.im_convt.weight)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_real = x[..., 0]\n",
    "        x_im = x[..., 1]\n",
    "\n",
    "        ct_real = self.real_convt(x_real) - self.im_convt(x_im)\n",
    "        ct_im = self.im_convt(x_real) + self.real_convt(x_im)\n",
    "\n",
    "        output = torch.stack([ct_real, ct_im], dim=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905fd64c",
   "metadata": {
    "id": "q-UT9p42B8DV",
    "papermill": {
     "duration": 0.004775,
     "end_time": "2025-08-22T14:54:28.326476",
     "exception": false,
     "start_time": "2025-08-22T14:54:28.321701",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Layer per la batch normalization di segnali complessi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b0c9379",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T14:54:28.336895Z",
     "iopub.status.busy": "2025-08-22T14:54:28.336696Z",
     "iopub.status.idle": "2025-08-22T14:54:28.342056Z",
     "shell.execute_reply": "2025-08-22T14:54:28.341392Z"
    },
    "id": "I3yANC1OCCIa",
    "papermill": {
     "duration": 0.011835,
     "end_time": "2025-08-22T14:54:28.343153",
     "exception": false,
     "start_time": "2025-08-22T14:54:28.331318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ComplexBatchNorm2d(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.affine = affine\n",
    "        self.track_running_stats = track_running_stats\n",
    "\n",
    "        self.real_b = nn.BatchNorm2d(num_features=self.num_features, eps=self.eps, momentum=self.momentum,\n",
    "                                      affine=self.affine, track_running_stats=self.track_running_stats)\n",
    "        self.im_b = nn.BatchNorm2d(num_features=self.num_features, eps=self.eps, momentum=self.momentum,\n",
    "                                    affine=self.affine, track_running_stats=self.track_running_stats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_real = x[..., 0]\n",
    "        x_im = x[..., 1]\n",
    "\n",
    "        n_real = self.real_b(x_real)\n",
    "        n_im = self.im_b(x_im)\n",
    "\n",
    "        output = torch.stack([n_real, n_im], dim=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001f80f5",
   "metadata": {
    "id": "2odkCO2UCoDo",
    "papermill": {
     "duration": 0.004622,
     "end_time": "2025-08-22T14:54:28.352618",
     "exception": false,
     "start_time": "2025-08-22T14:54:28.347996",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Layer Encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59b6ecde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T14:54:28.363142Z",
     "iopub.status.busy": "2025-08-22T14:54:28.362729Z",
     "iopub.status.idle": "2025-08-22T14:54:28.367335Z",
     "shell.execute_reply": "2025-08-22T14:54:28.366860Z"
    },
    "id": "XgUrwf8cCr45",
    "papermill": {
     "duration": 0.010964,
     "end_time": "2025-08-22T14:54:28.368316",
     "exception": false,
     "start_time": "2025-08-22T14:54:28.357352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, filter_size=(7,5), stride_size=(2,2), in_channels=1, out_channels=45, padding=(0,0)):\n",
    "        super().__init__()\n",
    "\n",
    "        self.filter_size = filter_size\n",
    "        self.stride_size = stride_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.padding = padding\n",
    "\n",
    "        self.cconv = ComplexConv2d(in_channels=self.in_channels, out_channels=self.out_channels,\n",
    "                             kernel_size=self.filter_size, stride=self.stride_size, padding=self.padding)\n",
    "\n",
    "        self.cbn = ComplexBatchNorm2d(num_features=self.out_channels)\n",
    "\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        conved = self.cconv(x)\n",
    "        normed = self.cbn(conved)\n",
    "        acted = self.leaky_relu(normed)\n",
    "\n",
    "        return acted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec46b533",
   "metadata": {
    "id": "t_mwsQ_bDJsD",
    "papermill": {
     "duration": 0.004763,
     "end_time": "2025-08-22T14:54:28.377978",
     "exception": false,
     "start_time": "2025-08-22T14:54:28.373215",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Layer Decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f1d2b27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T14:54:28.388475Z",
     "iopub.status.busy": "2025-08-22T14:54:28.388264Z",
     "iopub.status.idle": "2025-08-22T14:54:28.393976Z",
     "shell.execute_reply": "2025-08-22T14:54:28.393302Z"
    },
    "id": "4OMBVSdbDMBV",
    "papermill": {
     "duration": 0.012132,
     "end_time": "2025-08-22T14:54:28.395024",
     "exception": false,
     "start_time": "2025-08-22T14:54:28.382892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, filter_size=(7,5), stride_size=(2,2), in_channels=1, out_channels=45,\n",
    "                 output_padding=(0,0), padding=(0,0), last_layer=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.filter_size = filter_size\n",
    "        self.stride_size = stride_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.output_padding = output_padding\n",
    "        self.padding = padding\n",
    "\n",
    "        self.last_layer = last_layer\n",
    "\n",
    "        self.cconvt = ComplexConvTranspose2d(in_channels=self.in_channels, out_channels=self.out_channels,\n",
    "                             kernel_size=self.filter_size, stride=self.stride_size, output_padding=self.output_padding, padding=self.padding)\n",
    "\n",
    "        self.cbn = ComplexBatchNorm2d(num_features=self.out_channels)\n",
    "\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        conved = self.cconvt(x)\n",
    "\n",
    "        if not self.last_layer:\n",
    "            normed = self.cbn(conved)\n",
    "            output = self.leaky_relu(normed)\n",
    "        else:\n",
    "            m_phase = conved / (torch.abs(conved) + 1e-8)\n",
    "            m_mag = torch.tanh(torch.abs(conved))\n",
    "            output = m_phase * m_mag\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f84e995",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T14:54:28.466344Z",
     "iopub.status.busy": "2025-08-22T14:54:28.465737Z",
     "iopub.status.idle": "2025-08-22T14:54:28.470243Z",
     "shell.execute_reply": "2025-08-22T14:54:28.469498Z"
    },
    "papermill": {
     "duration": 0.01107,
     "end_time": "2025-08-22T14:54:28.471423",
     "exception": false,
     "start_time": "2025-08-22T14:54:28.460353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stft_to_waveform(stft_tensor, n_fft=n_fft, hop_length=hop_length):\n",
    "    \"\"\"Converte STFT in waveform\"\"\"\n",
    "    if stft_tensor.dim() == 5:  # [batch, channel, freq, time, 2]\n",
    "        stft_tensor = torch.squeeze(stft_tensor, 1)\n",
    "    elif stft_tensor.dim() == 4:  # [batch, freq, time, 2]\n",
    "        pass\n",
    "    else:\n",
    "        print(f\"Unexpected tensor dimensions: {stft_tensor.shape}\")\n",
    "        return None\n",
    "    \n",
    "    # Converto in complesso\n",
    "    complex_tensor = torch.complex(stft_tensor[..., 0], stft_tensor[..., 1])\n",
    "    \n",
    "    # Applico ISTFT\n",
    "    waveform = torch.istft(complex_tensor, n_fft=n_fft, hop_length=hop_length, \n",
    "                          window=window, normalized=True)\n",
    "    return waveform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73218cdd",
   "metadata": {
    "id": "b5FhkO2JZ60q",
    "papermill": {
     "duration": 0.005176,
     "end_time": "2025-08-22T14:54:28.575281",
     "exception": false,
     "start_time": "2025-08-22T14:54:28.570105",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Modello a 20 layer della DCUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3df436f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T14:54:28.586741Z",
     "iopub.status.busy": "2025-08-22T14:54:28.586484Z",
     "iopub.status.idle": "2025-08-22T14:54:28.603320Z",
     "shell.execute_reply": "2025-08-22T14:54:28.602675Z"
    },
    "id": "NWQZYjNTaFWn",
    "papermill": {
     "duration": 0.023736,
     "end_time": "2025-08-22T14:54:28.604322",
     "exception": false,
     "start_time": "2025-08-22T14:54:28.580586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DCUnet20(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net complessa che predice maschera in STFT (real/imag). Output same-shape dello STFT input.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_fft=2048, hop_length=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "\n",
    "        self.set_size(model_complexity=32, input_channels=1, model_depth=20)\n",
    "\n",
    "        # costruzione degli encoder\n",
    "        self.encoders = []\n",
    "        self.model_length = 20 // 2  # → 10 encoder e 10 decoder\n",
    "\n",
    "        for i in range(self.model_length):\n",
    "            module = Encoder(in_channels=self.enc_channels[i], out_channels=self.enc_channels[i + 1],\n",
    "                             filter_size=self.enc_kernel_sizes[i], stride_size=self.enc_strides[i], padding=self.enc_paddings[i])\n",
    "            self.add_module(\"encoder{}\".format(i), module)\n",
    "            self.encoders.append(module)\n",
    "\n",
    "        # costruzione dei decoder\n",
    "        self.decoders = []\n",
    "        \n",
    "        for i in range(self.model_length):\n",
    "            if i != self.model_length - 1:\n",
    "                # Il primo decoder non deve sommare le skip connections nell'input\n",
    "                if i == 0:\n",
    "                    # Primo decoder: solo i canali dall'encoder finale\n",
    "                    in_channels = self.dec_channels[i]\n",
    "                else:\n",
    "                    # Altri decoder: canali decoder + skip connection\n",
    "                    in_channels = self.dec_channels[i] + self.enc_channels[self.model_length - i]\n",
    "                    \n",
    "                module = Decoder(in_channels=in_channels, \n",
    "                                out_channels=self.dec_channels[i + 1],\n",
    "                                filter_size=self.dec_kernel_sizes[i], \n",
    "                                stride_size=self.dec_strides[i], \n",
    "                                padding=self.dec_paddings[i],\n",
    "                                output_padding=self.dec_output_padding[i])\n",
    "            else:\n",
    "                # Ultimo decoder\n",
    "                in_channels = self.dec_channels[i] + self.enc_channels[self.model_length - i]\n",
    "                module = Decoder(in_channels=in_channels, \n",
    "                                out_channels=self.dec_channels[i + 1],\n",
    "                                filter_size=self.dec_kernel_sizes[i], \n",
    "                                stride_size=self.dec_strides[i], \n",
    "                                padding=self.dec_paddings[i],\n",
    "                                output_padding=self.dec_output_padding[i], \n",
    "                                last_layer=True)\n",
    "            \n",
    "            self.add_module(\"decoder{}\".format(i), module)\n",
    "            self.decoders.append(module)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, is_istft=True):\n",
    "        orig_x = x\n",
    "        xs = []\n",
    "        \n",
    "        # Controllo che l'input abbia dimensioni corrette\n",
    "        if x.dim() == 4:  # [batch, freq, time, 2]\n",
    "            x = x.unsqueeze(1)  # [batch, 1, freq, time, 2]\n",
    "        \n",
    "        # Encoder (mantieni dimensioni consistenti)\n",
    "        for i, encoder in enumerate(self.encoders):\n",
    "            xs.append(x)\n",
    "            x = encoder(x)\n",
    "            #print(f\"Encoder {i}: {x.shape}\")\n",
    "        \n",
    "        # Decoder con controlli\n",
    "        p = x\n",
    "        for i, decoder in enumerate(self.decoders):\n",
    "            p = decoder(p)\n",
    "            #print(f\"Decoder {i} output: {p.shape}\")\n",
    "            \n",
    "            if i < self.model_length - 1:\n",
    "                skip_connection = xs[self.model_length - 1 - i]\n",
    "                #print(f\"Skip connection {i}: {skip_connection.shape}\")\n",
    "                \n",
    "                # CONTROLLO delle dimensioni\n",
    "                if p.shape[2:4] != skip_connection.shape[2:4]:\n",
    "                    print(f\"ERRORE: Dimensioni incompatibili!\")\n",
    "                    print(f\"Decoder: {p.shape}, Skip: {skip_connection.shape}\")\n",
    "                    return torch.zeros_like(orig_x)\n",
    "                \n",
    "                p = torch.cat([p, skip_connection], dim=1)\n",
    "        \n",
    "        # Output finale\n",
    "        mask = p\n",
    "        output = mask * orig_x.unsqueeze(1) if orig_x.dim() == 4 else mask * orig_x\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def set_size(self, model_complexity, model_depth=20, input_channels=1):\n",
    "      # definisce tutte le dimensioni e i parametri per encoder e decoder, specifici per la versione a 20 layer\n",
    "\n",
    "        if model_depth == 20:\n",
    "            self.enc_channels = [input_channels,\n",
    "                                 model_complexity,\n",
    "                                 model_complexity,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 4]\n",
    "\n",
    "            self.enc_kernel_sizes = [(7, 1),\n",
    "                                     (1, 7),\n",
    "                                     (6, 4),\n",
    "                                     (7, 5),\n",
    "                                     (5, 3),\n",
    "                                     (5, 3),\n",
    "                                     (5, 3),\n",
    "                                     (5, 3),\n",
    "                                     (5, 3),\n",
    "                                     (5, 3)]\n",
    "\n",
    "            self.enc_strides = [(1, 1),\n",
    "                                (1, 1),\n",
    "                                (2, 2),\n",
    "                                (2, 1),\n",
    "                                (2, 2),\n",
    "                                (2, 1),\n",
    "                                (2, 2),\n",
    "                                (2, 1),\n",
    "                                (2, 2),\n",
    "                                (2, 1)]\n",
    "\n",
    "            self.enc_paddings = [(3, 0),\n",
    "                                 (0, 3),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (2, 0),\n",
    "                                 (0, 0)]\n",
    "\n",
    "            self.dec_channels = [model_complexity * 4,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity,\n",
    "                                 model_complexity,\n",
    "                                 input_channels]\n",
    "\n",
    "            self.dec_kernel_sizes = [(6, 3),\n",
    "                                     (3, 3),\n",
    "                                     (6, 3),\n",
    "                                     (6, 3),\n",
    "                                     (6, 3),\n",
    "                                     (6, 4),\n",
    "                                     (8, 5),\n",
    "                                     (7, 5),\n",
    "                                     (1, 7),\n",
    "                                     (7, 1)]\n",
    "\n",
    "            self.dec_strides = [(2, 1), #\n",
    "                                (2, 2), #\n",
    "                                (2, 1), #\n",
    "                                (2, 2), #\n",
    "                                (2, 1), #\n",
    "                                (2, 2), #\n",
    "                                (2, 1), #\n",
    "                                (2, 2), #\n",
    "                                (1, 1),\n",
    "                                (1, 1)]\n",
    "\n",
    "            self.dec_paddings = [(0, 0),\n",
    "                                 (1, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 3),\n",
    "                                 (3, 0)]\n",
    "\n",
    "            self.dec_output_padding = [(0,0),\n",
    "                                       (1,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0)]\n",
    "        else:\n",
    "            raise ValueError(\"Unknown model depth : {}\".format(model_depth))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79bad34c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T14:54:28.615016Z",
     "iopub.status.busy": "2025-08-22T14:54:28.614822Z",
     "iopub.status.idle": "2025-08-22T14:54:28.618783Z",
     "shell.execute_reply": "2025-08-22T14:54:28.618028Z"
    },
    "papermill": {
     "duration": 0.010558,
     "end_time": "2025-08-22T14:54:28.619978",
     "exception": false,
     "start_time": "2025-08-22T14:54:28.609420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def debug_shapes(model, sample_input):\n",
    "    \"\"\"Debug delle dimensioni attraverso il modello\"\"\"\n",
    "    #print(f\"Input shape: {sample_input.shape}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(sample_input, is_istft=False)\n",
    "        #print(f\"Output shape: {output.shape}\")\n",
    "        \n",
    "        if output.shape != sample_input.shape:\n",
    "            print(\"ERRORE: Output != Input shape!\")\n",
    "            return False\n",
    "        else:\n",
    "            #print(\"Dimensioni corrette\")\n",
    "            return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02dabe2",
   "metadata": {
    "id": "9ay63BHhPQhd",
    "papermill": {
     "duration": 0.004838,
     "end_time": "2025-08-22T14:54:28.629732",
     "exception": false,
     "start_time": "2025-08-22T14:54:28.624894",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prova il denoising su un'intera canzone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5ca0f7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T14:54:28.640377Z",
     "iopub.status.busy": "2025-08-22T14:54:28.640172Z",
     "iopub.status.idle": "2025-08-22T14:54:43.965336Z",
     "shell.execute_reply": "2025-08-22T14:54:43.964584Z"
    },
    "papermill": {
     "duration": 15.331914,
     "end_time": "2025-08-22T14:54:43.966451",
     "exception": false,
     "start_time": "2025-08-22T14:54:28.634537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DCUnet20 Audio Denoising\n",
      "==================================================\n",
      "\n",
      "Caricamento modello...\n",
      "Modello caricato con successo!\n",
      "\n",
      "Processamento audio...\n",
      "Caricamento audio da: /kaggle/input/musdb18-whitenoiseonly/test/input/INPUT-S10N1-(WHITE_NOISE)-(forkupines-semantics).wav\n",
      "Audio totale: 273.39 secondi\n",
      "Chunk size: 164934 samples (3.74s)\n",
      "Overlap: 49480 samples\n",
      "Processamento 104 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104/104 [00:13<00:00,  7.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvataggio audio denoised in: /kaggle/working/canzone_denoised.wav\n",
      "\n",
      "==================================================\n",
      "Processo completato!\n",
      "Audio denoised salvato in: /kaggle/working/canzone_denoised.wav\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "CHUNK_DURATION = 3.74   # Processa audio in chunk di 10 secondi\n",
    "OVERLAP = 0.3  # Overlap del 50% tra chunk per evitare artefatti\n",
    "\n",
    "# Path del modello pre-allenato\n",
    "MODEL_PATH = \"/kaggle/input/white/pytorch/default/1/dc20_white_best.pth\"    # MODIFICA CON IL TUO PATH\n",
    "INPUT_AUDIO_PATH = \"/kaggle/input/musdb18-whitenoiseonly/test/input/INPUT-S10N1-(WHITE_NOISE)-(forkupines-semantics).wav\"  # MODIFICA CON IL TUO FILE\n",
    "OUTPUT_AUDIO_PATH = \"/kaggle/working/canzone_denoised.wav\"\n",
    "\n",
    "# =====================================================\n",
    "# FUNZIONI HELPER\n",
    "# =====================================================\n",
    "\n",
    "def load_audio(file_path, sample_rate=SAMPLE_RATE):\n",
    "    \"\"\"Carica file audio e converte a mono se necessario\"\"\"\n",
    "    waveform, sr = torchaudio.load(file_path)\n",
    "    if sr != sample_rate:\n",
    "        resampler = torchaudio.transforms.Resample(sr, sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    return waveform, sample_rate\n",
    "\n",
    "def process_chunk(model, chunk_waveform, window):\n",
    "    \"\"\"Processa un singolo chunk di audio\"\"\"\n",
    "    # STFT\n",
    "    stft = torch.stft(chunk_waveform, n_fft=n_fft, hop_length=hop_length,\n",
    "                     window=window, normalized=True, return_complex=True)\n",
    "    stft = torch.view_as_real(stft)\n",
    "    \n",
    "    # Aggiungi dimensione batch\n",
    "    stft = stft.unsqueeze(0)\n",
    "    \n",
    "    # Applica modello\n",
    "    with torch.no_grad():\n",
    "        pred_stft = model(stft, is_istft=False)\n",
    "    \n",
    "    # Rimuovi dimensione batch e channel se presenti\n",
    "    if pred_stft.dim() == 5:\n",
    "        pred_stft = pred_stft.squeeze(0).squeeze(0)\n",
    "    elif pred_stft.dim() == 4:\n",
    "        pred_stft = pred_stft.squeeze(0)\n",
    "    \n",
    "    # ISTFT\n",
    "    complex_tensor = torch.complex(pred_stft[..., 0], pred_stft[..., 1])\n",
    "    denoised_chunk = torch.istft(complex_tensor, n_fft=n_fft, hop_length=hop_length,\n",
    "                                window=window, normalized=True)\n",
    "    \n",
    "    return denoised_chunk\n",
    "\n",
    "def denoise_long_audio(model, audio_path, output_path):\n",
    "    \"\"\"Processa un file audio lungo dividendolo in chunk\"\"\"\n",
    "    \n",
    "    print(f\"Caricamento audio da: {audio_path}\")\n",
    "    waveform, sr = load_audio(audio_path)\n",
    "    waveform = waveform.to(device)\n",
    "    \n",
    "    total_samples = waveform.shape[-1]\n",
    "    chunk_samples = int(CHUNK_DURATION * sr)\n",
    "    overlap_samples = int(chunk_samples * OVERLAP)\n",
    "    hop_samples = chunk_samples - overlap_samples\n",
    "    \n",
    "    print(f\"Audio totale: {total_samples/sr:.2f} secondi\")\n",
    "    print(f\"Chunk size: {chunk_samples} samples ({CHUNK_DURATION}s)\")\n",
    "    print(f\"Overlap: {overlap_samples} samples\")\n",
    "    \n",
    "    \n",
    "    # Buffer per output\n",
    "    output_waveform = torch.zeros_like(waveform)\n",
    "    weight_sum = torch.zeros_like(waveform)\n",
    "    \n",
    "    # Finestra di fade per overlap\n",
    "    fade_window = torch.hann_window(chunk_samples).to(device)\n",
    "    \n",
    "    # Processa chunk per chunk\n",
    "    num_chunks = (total_samples - overlap_samples) // hop_samples + 1\n",
    "    \n",
    "    print(f\"Processamento {num_chunks} chunks...\")\n",
    "    for i in tqdm(range(num_chunks)):\n",
    "        start_idx = i * hop_samples\n",
    "        end_idx = min(start_idx + chunk_samples, total_samples)\n",
    "        \n",
    "        # Estrai chunk\n",
    "        chunk = waveform[:, start_idx:end_idx]\n",
    "        \n",
    "        # Pad se necessario (ultimo chunk)\n",
    "        if chunk.shape[-1] < chunk_samples:\n",
    "            padding = chunk_samples - chunk.shape[-1]\n",
    "            chunk = torch.nn.functional.pad(chunk, (0, padding))\n",
    "        \n",
    "        # Processa chunk\n",
    "        denoised_chunk = process_chunk(model, chunk, window)\n",
    "        \n",
    "        # Assicura dimensioni corrette\n",
    "        if denoised_chunk.dim() == 1:\n",
    "            denoised_chunk = denoised_chunk.unsqueeze(0)\n",
    "        \n",
    "        # Taglia il padding se necessario\n",
    "        actual_length = min(end_idx - start_idx, denoised_chunk.shape[-1])\n",
    "        denoised_chunk = denoised_chunk[:, :actual_length]\n",
    "        \n",
    "        # Applica fade window per smooth blending\n",
    "        fade_length = min(actual_length, fade_window.shape[0])\n",
    "        denoised_chunk[:, :fade_length] *= fade_window[:fade_length]\n",
    "        \n",
    "        # Accumula nell'output con overlap-add\n",
    "        output_waveform[:, start_idx:start_idx+actual_length] += denoised_chunk\n",
    "        weight_sum[:, start_idx:start_idx+actual_length] += fade_window[:fade_length]\n",
    "    \n",
    "    # Normalizza per i pesi\n",
    "    mask = weight_sum > 0\n",
    "    output_waveform[mask] /= weight_sum[mask]\n",
    "    \n",
    "    # Normalizza audio per evitare clipping\n",
    "    max_val = torch.abs(output_waveform).max()\n",
    "    if max_val > 0.95:\n",
    "        output_waveform = output_waveform * 0.95 / max_val\n",
    "    \n",
    "    # Salva audio\n",
    "    print(f\"Salvataggio audio denoised in: {output_path}\")\n",
    "    torchaudio.save(output_path, output_waveform.cpu(), sr)\n",
    "    \n",
    "    return output_waveform\n",
    "\n",
    "# =====================================================\n",
    "# MAIN\n",
    "# =====================================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\"*50)\n",
    "    print(\"DCUnet20 Audio Denoising\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Carica modello\n",
    "    print(\"\\nCaricamento modello...\")\n",
    "    model = DCUnet20(n_fft=n_fft, hop_length=hop_length).to(device)\n",
    "    \n",
    "    # Carica i pesi\n",
    "    checkpoint = torch.load(MODEL_PATH, map_location=device)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model.eval()\n",
    "    print(\"Modello caricato con successo!\")\n",
    "    \n",
    "    # Processa audio\n",
    "    print(\"\\nProcessamento audio...\")\n",
    "    denoised_waveform = denoise_long_audio(model, INPUT_AUDIO_PATH, OUTPUT_AUDIO_PATH)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Processo completato!\")\n",
    "    print(f\"Audio denoised salvato in: {OUTPUT_AUDIO_PATH}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7818922,
     "sourceId": 12398942,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 433436,
     "modelInstanceId": 415685,
     "sourceId": 532275,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 25.450785,
   "end_time": "2025-08-22T14:54:45.095563",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-22T14:54:19.644778",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
