{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9oX5DLS3COS5"
   },
   "source": [
    "# Speech Denoising without Clean Training Data: a Noise2Noise Approach #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RETE DEL PAPER ORIGINALE CORRETTA, USATA PER OTTENERE LE VALUTAZIONI CON IL NOSTRO NUOVO DATASET DI CANZONI\n",
    "\n",
    "Link ai pesi originali della rete preallenata: https://www.kaggle.com/datasets/frads01/pretrainedweights\n",
    "\n",
    "Link al dataset originale speech: https://www.kaggle.com/datasets/frads01/refdataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter the noise type you want to train the model to denoise. The test and train dataset must already be generated beforehand. ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### white : additive_gaussian_noise ###\n",
    "### 0 : air_conditioner ###\n",
    "### 1 : car_horn ###\n",
    "### 2 : children_playing ###\n",
    "### 3 : dog_bark ###\n",
    "### 4 : drilling ###\n",
    "### 5 : engine_idling ###\n",
    "### 6 : gun_shot ###\n",
    "### 7 : jackhammer ###\n",
    "### 8 : siren ###\n",
    "### 9 : street_music ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:28.250576Z",
     "iopub.status.busy": "2025-08-18T14:45:28.249994Z",
     "iopub.status.idle": "2025-08-18T14:45:28.257350Z",
     "shell.execute_reply": "2025-08-18T14:45:28.256532Z",
     "shell.execute_reply.started": "2025-08-18T14:45:28.250544Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "noise_class = \"white\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the type of training you want to employ: either \"Noise2Noise\" or \"Noise2Clean\"  ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:28.259275Z",
     "iopub.status.busy": "2025-08-18T14:45:28.258729Z",
     "iopub.status.idle": "2025-08-18T14:45:28.278040Z",
     "shell.execute_reply": "2025-08-18T14:45:28.277248Z",
     "shell.execute_reply.started": "2025-08-18T14:45:28.259254Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_type =  \"Noise2Noise\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FiXUioRtCOS6"
   },
   "source": [
    "### Import of libraries ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:28.279405Z",
     "iopub.status.busy": "2025-08-18T14:45:28.279154Z",
     "iopub.status.idle": "2025-08-18T14:45:28.293164Z",
     "shell.execute_reply": "2025-08-18T14:45:28.292559Z",
     "shell.execute_reply.started": "2025-08-18T14:45:28.279380Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "if noise_class == \"white\": \n",
    "    TRAIN_INPUT_DIR = Path('/kaggle/input/refdataset/Datasets/WhiteNoise_Train_Input')\n",
    "\n",
    "    if training_type == \"Noise2Noise\":\n",
    "        TRAIN_TARGET_DIR = Path('/kaggle/input/refdataset/Datasets/WhiteNoise_Train_Output')\n",
    "    elif training_type == \"Noise2Clean\":\n",
    "        TRAIN_TARGET_DIR = Path('/kaggle/input/refdataset/Datasets/clean_trainset_28spk_wav')\n",
    "    else:\n",
    "        raise Exception(\"Enter valid training type\")\n",
    "\n",
    "    TEST_NOISY_DIR = Path('/kaggle/input/refdataset/Datasets/WhiteNoise_Test_Input')\n",
    "    TEST_CLEAN_DIR = Path('/kaggle/input/refdataset/Datasets/clean_testset_wav') \n",
    "    \n",
    "else:\n",
    "    TRAIN_INPUT_DIR = Path('/kaggle/input/refdataset/Datasets/US_Class'+str(noise_class)+'_Train_Input')\n",
    "\n",
    "    if training_type == \"Noise2Noise\":\n",
    "        TRAIN_TARGET_DIR = Path('/kaggle/input/refdataset/Datasets/US_Class'+str(noise_class)+'_Train_Output')\n",
    "    elif training_type == \"Noise2Clean\":\n",
    "        TRAIN_TARGET_DIR = Path('/kaggle/input/refdataset/Datasets/clean_trainset_28spk_wav')\n",
    "    else:\n",
    "        raise Exception(\"Enter valid training type\")\n",
    "\n",
    "    TEST_NOISY_DIR = Path('/kaggle/input/refdataset/Datasets/US_Class'+str(noise_class)+'_Test_Input')\n",
    "    TEST_CLEAN_DIR = Path('/kaggle/input/refdataset/Datasets/clean_testset_wav') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:28.345856Z",
     "iopub.status.busy": "2025-08-18T14:45:28.345603Z",
     "iopub.status.idle": "2025-08-18T14:45:28.350532Z",
     "shell.execute_reply": "2025-08-18T14:45:28.349693Z",
     "shell.execute_reply.started": "2025-08-18T14:45:28.345836Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "basepath = str(noise_class)+\"_\"+training_type\n",
    "os.makedirs(basepath,exist_ok=True)\n",
    "os.makedirs(basepath+\"/Weights\",exist_ok=True)\n",
    "os.makedirs(basepath+\"/Samples\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:28.358546Z",
     "iopub.status.busy": "2025-08-18T14:45:28.358361Z",
     "iopub.status.idle": "2025-08-18T14:45:34.240356Z",
     "shell.execute_reply": "2025-08-18T14:45:34.239236Z",
     "shell.execute_reply.started": "2025-08-18T14:45:28.358532Z"
    },
    "id": "D0bWtt2J5i9F",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import warnings\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from matplotlib import colors, pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# not everything is smooth in sklearn, to conveniently output images in colab\n",
    "# we will ignore warnings\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:34.242227Z",
     "iopub.status.busy": "2025-08-18T14:45:34.241913Z",
     "iopub.status.idle": "2025-08-18T14:45:34.250557Z",
     "shell.execute_reply": "2025-08-18T14:45:34.249783Z",
     "shell.execute_reply.started": "2025-08-18T14:45:34.242207Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(999)\n",
    "torch.manual_seed(999)\n",
    "\n",
    "# If running on Cuda set these 2 for determinism\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xnlgTsICOTA"
   },
   "source": [
    "### Checking whether the GPU is available ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:34.251479Z",
     "iopub.status.busy": "2025-08-18T14:45:34.251242Z",
     "iopub.status.idle": "2025-08-18T14:45:34.319308Z",
     "shell.execute_reply": "2025-08-18T14:45:34.318620Z",
     "shell.execute_reply.started": "2025-08-18T14:45:34.251461Z"
    },
    "id": "byUnPtQ25i9O",
    "outputId": "4b6d9f85-e5b6-4cb5-c3db-2f48321ed391",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# First checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')\n",
    "       \n",
    "DEVICE = torch.device('cuda' if train_on_gpu else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:34.321193Z",
     "iopub.status.busy": "2025-08-18T14:45:34.320998Z",
     "iopub.status.idle": "2025-08-18T14:45:34.507554Z",
     "shell.execute_reply": "2025-08-18T14:45:34.506644Z",
     "shell.execute_reply.started": "2025-08-18T14:45:34.321177Z"
    },
    "id": "qacrfNwA6vw_",
    "outputId": "e1183ab5-4f39-478a-b00a-74fe5edfb511",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N4AFJANDcBG"
   },
   "source": [
    "### Set Audio backend as Soundfile for windows and Sox for Linux ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:34.509150Z",
     "iopub.status.busy": "2025-08-18T14:45:34.508823Z",
     "iopub.status.idle": "2025-08-18T14:45:34.515105Z",
     "shell.execute_reply": "2025-08-18T14:45:34.514228Z",
     "shell.execute_reply.started": "2025-08-18T14:45:34.509115Z"
    },
    "id": "_N4AFJANDcBG",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torchaudio.set_audio_backend(\"soundfile\")\n",
    "print(\"TorchAudio backend used:\\t{}\".format(torchaudio.get_audio_backend()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWvlIABzCOTM"
   },
   "source": [
    "### The sampling frequency and the selected values for the Short-time Fourier transform. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:34.516206Z",
     "iopub.status.busy": "2025-08-18T14:45:34.515947Z",
     "iopub.status.idle": "2025-08-18T14:45:34.533136Z",
     "shell.execute_reply": "2025-08-18T14:45:34.532468Z",
     "shell.execute_reply.started": "2025-08-18T14:45:34.516179Z"
    },
    "id": "8ngFJtPj5i9V",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 48000\n",
    "N_FFT = (SAMPLE_RATE * 64) // 1000 \n",
    "HOP_LENGTH = (SAMPLE_RATE * 16) // 1000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8suREWkb5Se"
   },
   "source": [
    "### The declaration of datasets and dataloaders ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:34.534048Z",
     "iopub.status.busy": "2025-08-18T14:45:34.533834Z",
     "iopub.status.idle": "2025-08-18T14:45:34.552462Z",
     "shell.execute_reply": "2025-08-18T14:45:34.551913Z",
     "shell.execute_reply.started": "2025-08-18T14:45:34.534033Z"
    },
    "id": "cZ0wb9EN5i9f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SpeechDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset class with audio that cuts them/paddes them to a specified length, applies a Short-tome Fourier transform,\n",
    "    normalizes and leads to a tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self, noisy_files, clean_files, n_fft=64, hop_length=16):\n",
    "        super().__init__()\n",
    "        # list of files\n",
    "        self.noisy_files = sorted(noisy_files)\n",
    "        self.clean_files = sorted(clean_files)\n",
    "        \n",
    "        # stft parameters\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        \n",
    "        self.len_ = len(self.noisy_files)\n",
    "        \n",
    "        # fixed len\n",
    "        self.max_len = 165000\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len_\n",
    "      \n",
    "    def load_sample(self, file):\n",
    "        waveform, _ = torchaudio.load(file)\n",
    "        return waveform\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        # load to tensors and normalization\n",
    "        x_clean = self.load_sample(self.clean_files[index])\n",
    "        x_noisy = self.load_sample(self.noisy_files[index])\n",
    "        \n",
    "        # padding/cutting\n",
    "        x_clean = self._prepare_sample(x_clean)\n",
    "        x_noisy = self._prepare_sample(x_noisy)\n",
    "        \n",
    "        # Short-time Fourier transform - AGGIUNGERE return_complex=True\n",
    "        x_noisy_stft = torch.stft(input=x_noisy, n_fft=self.n_fft, \n",
    "                             hop_length=self.hop_length, normalized=True, \n",
    "                             return_complex=True)\n",
    "        x_clean_stft = torch.stft(input=x_clean, n_fft=self.n_fft, \n",
    "                             hop_length=self.hop_length, normalized=True, \n",
    "                             return_complex=True)\n",
    "    \n",
    "        # Convertire il tensore complesso in formato compatibile con il modello\n",
    "        x_noisy_stft = torch.view_as_real(x_noisy_stft)\n",
    "        x_clean_stft = torch.view_as_real(x_clean_stft)\n",
    "        \n",
    "        return x_noisy_stft, x_clean_stft\n",
    "        \n",
    "    def _prepare_sample(self, waveform):\n",
    "        waveform = waveform.numpy()\n",
    "        current_len = waveform.shape[1]\n",
    "        \n",
    "        output = np.zeros((1, self.max_len), dtype='float32')\n",
    "        output[0, -current_len:] = waveform[0, :self.max_len]\n",
    "        output = torch.from_numpy(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:34.553438Z",
     "iopub.status.busy": "2025-08-18T14:45:34.553210Z",
     "iopub.status.idle": "2025-08-18T14:45:34.609514Z",
     "shell.execute_reply": "2025-08-18T14:45:34.608982Z",
     "shell.execute_reply.started": "2025-08-18T14:45:34.553421Z"
    },
    "id": "8NLV2lcv5i9k",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_input_files = sorted(list(TRAIN_INPUT_DIR.rglob('*.wav')))\n",
    "train_target_files = sorted(list(TRAIN_TARGET_DIR.rglob('*.wav')))\n",
    "\n",
    "test_noisy_files = sorted(list(TEST_NOISY_DIR.rglob('*.wav')))\n",
    "test_clean_files = sorted(list(TEST_CLEAN_DIR.rglob('*.wav')))\n",
    "\n",
    "print(\"No. of Training files:\",len(train_input_files))\n",
    "print(\"No. of Testing files:\",len(test_noisy_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:34.610677Z",
     "iopub.status.busy": "2025-08-18T14:45:34.610377Z",
     "iopub.status.idle": "2025-08-18T14:45:34.614529Z",
     "shell.execute_reply": "2025-08-18T14:45:34.613954Z",
     "shell.execute_reply.started": "2025-08-18T14:45:34.610653Z"
    },
    "id": "7GHaKjYS5i9u",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_dataset = SpeechDataset(test_noisy_files, test_clean_files, N_FFT, HOP_LENGTH)\n",
    "train_dataset = SpeechDataset(train_input_files, train_target_files, N_FFT, HOP_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:34.615535Z",
     "iopub.status.busy": "2025-08-18T14:45:34.615289Z",
     "iopub.status.idle": "2025-08-18T14:45:34.631716Z",
     "shell.execute_reply": "2025-08-18T14:45:34.630860Z",
     "shell.execute_reply.started": "2025-08-18T14:45:34.615497Z"
    },
    "id": "79nIQjht5i9y",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# For testing purpose\n",
    "test_loader_single_unshuffled = DataLoader(test_dataset, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHJZXeUQcsrq"
   },
   "source": [
    "### Average Test Set Metrics ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:34.634662Z",
     "iopub.status.busy": "2025-08-18T14:45:34.634411Z",
     "iopub.status.idle": "2025-08-18T14:45:34.650912Z",
     "shell.execute_reply": "2025-08-18T14:45:34.650183Z",
     "shell.execute_reply.started": "2025-08-18T14:45:34.634629Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test_set_metrics(test_loader, model):\n",
    "    metric_names = [\"CSIG\",\"CBAK\",\"COVL\",\"PESQ\",\"SSNR\",\"STOI\"]\n",
    "    overall_metrics = [[] for i in range(len(metric_names))]\n",
    "    \n",
    "    for i,(noisy,clean) in enumerate(test_loader):\n",
    "        x_est = model(noisy.to(DEVICE), is_istft=True)\n",
    "        x_est_np = x_est[0].view(-1).detach().cpu().numpy()\n",
    "        x_c_np = torch.istft(torch.squeeze(clean[0], 1), n_fft=N_FFT, hop_length=HOP_LENGTH, normalized=True).view(-1).detach().cpu().numpy()\n",
    "        metrics = AudioMetrics(x_c_np, x_est_np, SAMPLE_RATE)\n",
    "        \n",
    "        overall_metrics[0].append(metrics.CSIG)\n",
    "        overall_metrics[1].append(metrics.CBAK)\n",
    "        overall_metrics[2].append(metrics.COVL)\n",
    "        overall_metrics[3].append(metrics.PESQ)\n",
    "        overall_metrics[4].append(metrics.SSNR)\n",
    "        overall_metrics[5].append(metrics.STOI)\n",
    "    \n",
    "    metrics_dict = dict()\n",
    "    for i in range(len(metric_names)):\n",
    "        metrics_dict[metric_names[i]] ={'mean': np.mean(overall_metrics[i]), 'std_dev': np.std(overall_metrics[i])} \n",
    "    \n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k73YEkgQCOTj"
   },
   "source": [
    "### Declaring the class layers ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:34.651876Z",
     "iopub.status.busy": "2025-08-18T14:45:34.651669Z",
     "iopub.status.idle": "2025-08-18T14:45:34.665088Z",
     "shell.execute_reply": "2025-08-18T14:45:34.664366Z",
     "shell.execute_reply.started": "2025-08-18T14:45:34.651860Z"
    },
    "id": "Znx7QM3h5i92",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CConv2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Class of complex valued convolutional layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.real_conv = nn.Conv2d(in_channels=self.in_channels, \n",
    "                                   out_channels=self.out_channels, \n",
    "                                   kernel_size=self.kernel_size, \n",
    "                                   padding=self.padding, \n",
    "                                   stride=self.stride)\n",
    "        \n",
    "        self.im_conv = nn.Conv2d(in_channels=self.in_channels, \n",
    "                                 out_channels=self.out_channels, \n",
    "                                 kernel_size=self.kernel_size, \n",
    "                                 padding=self.padding, \n",
    "                                 stride=self.stride)\n",
    "        \n",
    "        # Glorot initialization.\n",
    "        nn.init.xavier_uniform_(self.real_conv.weight)\n",
    "        nn.init.xavier_uniform_(self.im_conv.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_real = x[..., 0]\n",
    "        x_im = x[..., 1]\n",
    "        \n",
    "        c_real = self.real_conv(x_real) - self.im_conv(x_im)\n",
    "        c_im = self.im_conv(x_real) + self.real_conv(x_im)\n",
    "        \n",
    "        output = torch.stack([c_real, c_im], dim=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:34.665992Z",
     "iopub.status.busy": "2025-08-18T14:45:34.665785Z",
     "iopub.status.idle": "2025-08-18T14:45:34.685502Z",
     "shell.execute_reply": "2025-08-18T14:45:34.684863Z",
     "shell.execute_reply.started": "2025-08-18T14:45:34.665976Z"
    },
    "id": "GgtxJbSQ5i96",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CConvTranspose2d(nn.Module):\n",
    "    \"\"\"\n",
    "      Class of complex valued dilation convolutional layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, output_padding=0, padding=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.output_padding = output_padding\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.real_convt = nn.ConvTranspose2d(in_channels=self.in_channels, \n",
    "                                            out_channels=self.out_channels, \n",
    "                                            kernel_size=self.kernel_size, \n",
    "                                            output_padding=self.output_padding,\n",
    "                                            padding=self.padding,\n",
    "                                            stride=self.stride)\n",
    "        \n",
    "        self.im_convt = nn.ConvTranspose2d(in_channels=self.in_channels, \n",
    "                                            out_channels=self.out_channels, \n",
    "                                            kernel_size=self.kernel_size, \n",
    "                                            output_padding=self.output_padding, \n",
    "                                            padding=self.padding,\n",
    "                                            stride=self.stride)\n",
    "        \n",
    "        \n",
    "        # Glorot initialization.\n",
    "        nn.init.xavier_uniform_(self.real_convt.weight)\n",
    "        nn.init.xavier_uniform_(self.im_convt.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_real = x[..., 0]\n",
    "        x_im = x[..., 1]\n",
    "        \n",
    "        ct_real = self.real_convt(x_real) - self.im_convt(x_im)\n",
    "        ct_im = self.im_convt(x_real) + self.real_convt(x_im)\n",
    "        \n",
    "        output = torch.stack([ct_real, ct_im], dim=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:34.686392Z",
     "iopub.status.busy": "2025-08-18T14:45:34.686198Z",
     "iopub.status.idle": "2025-08-18T14:45:34.704483Z",
     "shell.execute_reply": "2025-08-18T14:45:34.703862Z",
     "shell.execute_reply.started": "2025-08-18T14:45:34.686377Z"
    },
    "id": "OJSmVrxp5i9-",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CBatchNorm2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Class of complex valued batch normalization layer\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.affine = affine\n",
    "        self.track_running_stats = track_running_stats\n",
    "        \n",
    "        self.real_b = nn.BatchNorm2d(num_features=self.num_features, eps=self.eps, momentum=self.momentum,\n",
    "                                      affine=self.affine, track_running_stats=self.track_running_stats)\n",
    "        self.im_b = nn.BatchNorm2d(num_features=self.num_features, eps=self.eps, momentum=self.momentum,\n",
    "                                    affine=self.affine, track_running_stats=self.track_running_stats) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_real = x[..., 0]\n",
    "        x_im = x[..., 1]\n",
    "        \n",
    "        n_real = self.real_b(x_real)\n",
    "        n_im = self.im_b(x_im)  \n",
    "        \n",
    "        output = torch.stack([n_real, n_im], dim=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:34.706092Z",
     "iopub.status.busy": "2025-08-18T14:45:34.705490Z",
     "iopub.status.idle": "2025-08-18T14:45:34.723294Z",
     "shell.execute_reply": "2025-08-18T14:45:34.722607Z",
     "shell.execute_reply.started": "2025-08-18T14:45:34.706075Z"
    },
    "id": "N7W37XMO5i-B",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Class of upsample block\n",
    "    \"\"\"\n",
    "    def __init__(self, filter_size=(7,5), stride_size=(2,2), in_channels=1, out_channels=45, padding=(0,0)):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.filter_size = filter_size\n",
    "        self.stride_size = stride_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.padding = padding\n",
    "\n",
    "        self.cconv = CConv2d(in_channels=self.in_channels, out_channels=self.out_channels, \n",
    "                             kernel_size=self.filter_size, stride=self.stride_size, padding=self.padding)\n",
    "        \n",
    "        self.cbn = CBatchNorm2d(num_features=self.out_channels) \n",
    "        \n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \n",
    "        conved = self.cconv(x)\n",
    "        normed = self.cbn(conved)\n",
    "        acted = self.leaky_relu(normed)\n",
    "        \n",
    "        return acted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:34.724961Z",
     "iopub.status.busy": "2025-08-18T14:45:34.724757Z",
     "iopub.status.idle": "2025-08-18T14:45:34.740164Z",
     "shell.execute_reply": "2025-08-18T14:45:34.739454Z",
     "shell.execute_reply.started": "2025-08-18T14:45:34.724946Z"
    },
    "id": "fuugYDZs5i-G",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Class of downsample block\n",
    "    \"\"\"\n",
    "    def __init__(self, filter_size=(7,5), stride_size=(2,2), in_channels=1, out_channels=45,\n",
    "                 output_padding=(0,0), padding=(0,0), last_layer=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.filter_size = filter_size\n",
    "        self.stride_size = stride_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.output_padding = output_padding\n",
    "        self.padding = padding\n",
    "        \n",
    "        self.last_layer = last_layer\n",
    "        \n",
    "        self.cconvt = CConvTranspose2d(in_channels=self.in_channels, out_channels=self.out_channels, \n",
    "                             kernel_size=self.filter_size, stride=self.stride_size, output_padding=self.output_padding, padding=self.padding)\n",
    "        \n",
    "        self.cbn = CBatchNorm2d(num_features=self.out_channels) \n",
    "        \n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \n",
    "        conved = self.cconvt(x)\n",
    "        \n",
    "        if not self.last_layer:\n",
    "            normed = self.cbn(conved)\n",
    "            output = self.leaky_relu(normed)\n",
    "        else:\n",
    "            m_phase = conved / (torch.abs(conved) + 1e-8)\n",
    "            m_mag = torch.tanh(torch.abs(conved))\n",
    "            output = m_phase * m_mag\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3G5CqR3-COT8"
   },
   "source": [
    "### Loss function ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:34.741759Z",
     "iopub.status.busy": "2025-08-18T14:45:34.740997Z",
     "iopub.status.idle": "2025-08-18T14:45:34.976743Z",
     "shell.execute_reply": "2025-08-18T14:45:34.976103Z",
     "shell.execute_reply.started": "2025-08-18T14:45:34.741741Z"
    },
    "id": "J71ny6expQeW",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from scipy import interpolate\n",
    "\n",
    "def resample(original, old_rate, new_rate):\n",
    "    if old_rate != new_rate:\n",
    "        duration = original.shape[0] / old_rate\n",
    "        time_old  = np.linspace(0, duration, original.shape[0])\n",
    "        time_new  = np.linspace(0, duration, int(original.shape[0] * new_rate / old_rate))\n",
    "        interpolator = interpolate.interp1d(time_old, original.T)\n",
    "        new_audio = interpolator(time_new).T\n",
    "        return new_audio\n",
    "    else:\n",
    "        return original\n",
    "\n",
    "\n",
    "def wsdr_fn(x_, y_pred_, y_true_, eps=1e-8):\n",
    "    # to time-domain waveform\n",
    "    y_true_ = torch.squeeze(y_true_, 1)\n",
    "    if y_true_.shape[-1] == 2:  # formato [..., 2]\n",
    "        y_true_complex = torch.view_as_complex(y_true_)\n",
    "        y_true = torch.istft(y_true_complex, n_fft=N_FFT, hop_length=HOP_LENGTH, \n",
    "                             normalized=True, onesided=True)\n",
    "    else:  # già complesso\n",
    "        y_true = torch.istft(y_true_, n_fft=N_FFT, hop_length=HOP_LENGTH, \n",
    "                             normalized=True, onesided=True)\n",
    "    x_ = torch.squeeze(x_, 1)\n",
    "    x = safe_istft(x_, N_FFT, HOP_LENGTH)\n",
    "    \n",
    "\n",
    "    y_pred = y_pred_.flatten(1)\n",
    "    y_true = y_true.flatten(1)\n",
    "    x = x.flatten(1)\n",
    "\n",
    "\n",
    "    def sdr_fn(true, pred, eps=1e-8):\n",
    "        num = torch.sum(true * pred, dim=1)\n",
    "        den = torch.norm(true, p=2, dim=1) * torch.norm(pred, p=2, dim=1)\n",
    "        return -(num / (den + eps))\n",
    "\n",
    "    # true and estimated noise\n",
    "    z_true = x - y_true\n",
    "    z_pred = x - y_pred\n",
    "\n",
    "    a = torch.sum(y_true**2, dim=1) / (torch.sum(y_true**2, dim=1) + torch.sum(z_true**2, dim=1) + eps)\n",
    "    wSDR = a * sdr_fn(y_true, y_pred) + (1 - a) * sdr_fn(z_true, z_pred)\n",
    "    return torch.mean(wSDR)\n",
    "\n",
    "wonky_samples = []\n",
    "\n",
    "def getMetricsonLoader(loader, net, use_net=True):\n",
    "    net.eval()\n",
    "    # Original test metrics\n",
    "    scale_factor = 32768\n",
    "    # metric_names = [\"CSIG\",\"CBAK\",\"COVL\",\"PESQ\",\"SSNR\",\"STOI\",\"SNR \"]\n",
    "    metric_names = [\"PESQ-WB\",\"PESQ-NB\",\"SNR\",\"SSNR\",\"STOI\"]\n",
    "    overall_metrics = [[] for i in range(5)]\n",
    "    for i, data in enumerate(loader):\n",
    "        if (i+1)%10==0:\n",
    "            end_str = \"\\n\"\n",
    "        else:\n",
    "            end_str = \",\"\n",
    "        #print(i,end=end_str)\n",
    "        if i in wonky_samples:\n",
    "            print(\"Something's up with this sample. Passing...\")\n",
    "        else:\n",
    "            noisy = data[0]\n",
    "            clean = data[1]\n",
    "            if use_net: # Forward of net returns the istft version\n",
    "                x_est = net(noisy.to(DEVICE), is_istft=True)\n",
    "                x_est_np = x_est.view(-1).detach().cpu().numpy()\n",
    "            else:\n",
    "                x_est_np = torch.istft(torch.squeeze(noisy, 1), n_fft=N_FFT, hop_length=HOP_LENGTH, normalized=True, onesided=True).view(-1).detach().cpu().numpy()\n",
    "            x_clean_np = torch.istft(torch.squeeze(clean, 1), n_fft=N_FFT, hop_length=HOP_LENGTH, normalized=True, onesided=True).view(-1).detach().cpu().numpy()\n",
    "            \n",
    "        \n",
    "            metrics = AudioMetrics2(x_clean_np, x_est_np, 48000)\n",
    "            \n",
    "            ref_wb = resample(x_clean_np, 48000, 16000)\n",
    "            deg_wb = resample(x_est_np, 48000, 16000)\n",
    "            pesq_wb = pesq(16000, ref_wb, deg_wb, 'wb')\n",
    "            \n",
    "            ref_nb = resample(x_clean_np, 48000, 8000)\n",
    "            deg_nb = resample(x_est_np, 48000, 8000)\n",
    "            pesq_nb = pesq(8000, ref_nb, deg_nb, 'nb')\n",
    "\n",
    "            #print(new_scores)\n",
    "            #print(metrics.PESQ, metrics.STOI)\n",
    "\n",
    "            overall_metrics[0].append(pesq_wb)\n",
    "            overall_metrics[1].append(pesq_nb)\n",
    "            overall_metrics[2].append(metrics.SNR)\n",
    "            overall_metrics[3].append(metrics.SSNR)\n",
    "            overall_metrics[4].append(metrics.STOI)\n",
    "    print()\n",
    "    print(\"Sample metrics computed\")\n",
    "    results = {}\n",
    "    for i in range(5):\n",
    "        temp = {}\n",
    "        temp[\"Mean\"] =  np.mean(overall_metrics[i])\n",
    "        temp[\"STD\"]  =  np.std(overall_metrics[i])\n",
    "        temp[\"Min\"]  =  min(overall_metrics[i])\n",
    "        temp[\"Max\"]  =  max(overall_metrics[i])\n",
    "        results[metric_names[i]] = temp\n",
    "    print(\"Averages computed\")\n",
    "    if use_net:\n",
    "        addon = \"(cleaned by model)\"\n",
    "    else:\n",
    "        addon = \"(pre denoising)\"\n",
    "    print(\"Metrics on test data\",addon)\n",
    "    for i in range(5):\n",
    "        print(\"{} : {:.3f}+/-{:.3f}\".format(metric_names[i], np.mean(overall_metrics[i]), np.std(overall_metrics[i])))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYvWz_6jRZ3e"
   },
   "source": [
    "### Description of the training of epochs. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:34.977684Z",
     "iopub.status.busy": "2025-08-18T14:45:34.977358Z",
     "iopub.status.idle": "2025-08-18T14:45:34.982871Z",
     "shell.execute_reply": "2025-08-18T14:45:34.982083Z",
     "shell.execute_reply.started": "2025-08-18T14:45:34.977667Z"
    },
    "id": "VukJTCGIZ8ZU",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(net, train_loader, loss_fn, optimizer):\n",
    "    net.train()\n",
    "    train_ep_loss = 0.\n",
    "    counter = 0\n",
    "    for noisy_x, clean_x in train_loader:\n",
    "\n",
    "        noisy_x, clean_x = noisy_x.to(DEVICE), clean_x.to(DEVICE)\n",
    "\n",
    "        # zero  gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        pred_x = net(noisy_x)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = loss_fn(noisy_x, pred_x, clean_x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_ep_loss += loss.item() \n",
    "        counter += 1\n",
    "\n",
    "    train_ep_loss /= counter\n",
    "\n",
    "    # clear cache\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return train_ep_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYCJWaTMRgS3"
   },
   "source": [
    "### Description of the validation of epochs ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:34.983823Z",
     "iopub.status.busy": "2025-08-18T14:45:34.983584Z",
     "iopub.status.idle": "2025-08-18T14:45:35.001440Z",
     "shell.execute_reply": "2025-08-18T14:45:35.000880Z",
     "shell.execute_reply.started": "2025-08-18T14:45:34.983800Z"
    },
    "id": "-JKrTMpPhw19",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test_epoch(net, test_loader, loss_fn, use_net=True):\n",
    "    net.eval()\n",
    "    test_ep_loss = 0.\n",
    "    counter = 0.\n",
    "    '''\n",
    "    for noisy_x, clean_x in test_loader:\n",
    "        # get the output from the model\n",
    "        noisy_x, clean_x = noisy_x.to(DEVICE), clean_x.to(DEVICE)\n",
    "        pred_x = net(noisy_x)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = loss_fn(noisy_x, pred_x, clean_x)\n",
    "        # Calc the metrics here\n",
    "        test_ep_loss += loss.item() \n",
    "        \n",
    "        counter += 1\n",
    "\n",
    "    test_ep_loss /= counter\n",
    "    '''\n",
    "    \n",
    "    #print(\"Actual compute done...testing now\")\n",
    "    \n",
    "    testmet = getMetricsonLoader(test_loader,net,use_net)\n",
    "\n",
    "    # clear cache\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return test_ep_loss, testmet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "879vq_uBRm_2"
   },
   "source": [
    "### To understand whether the network is being trained or not, we will output a train and test loss. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:35.002438Z",
     "iopub.status.busy": "2025-08-18T14:45:35.002188Z",
     "iopub.status.idle": "2025-08-18T14:45:35.019717Z",
     "shell.execute_reply": "2025-08-18T14:45:35.019110Z",
     "shell.execute_reply.started": "2025-08-18T14:45:35.002416Z"
    },
    "id": "I4gdVmhRr1Qi",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(net, train_loader, test_loader, loss_fn, optimizer, scheduler, epochs):\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    for e in tqdm(range(epochs)):\n",
    "\n",
    "        # first evaluating for comparison\n",
    "        \n",
    "        if e == 0 and training_type==\"Noise2Clean\":\n",
    "            print(\"Pre-training evaluation\")\n",
    "            #with torch.no_grad():\n",
    "            #    test_loss,testmet = test_epoch(net, test_loader, loss_fn,use_net=False)\n",
    "            #print(\"Had to load model.. checking if deets match\")\n",
    "            testmet = getMetricsonLoader(test_loader,net,False)    # again, modified cuz im loading\n",
    "            #test_losses.append(test_loss)\n",
    "            #print(\"Loss before training:{:.6f}\".format(test_loss))\n",
    "        \n",
    "            with open(basepath + \"/results.txt\",\"w+\") as f:\n",
    "                f.write(\"Initial : \\n\")\n",
    "                f.write(str(testmet))\n",
    "                f.write(\"\\n\")\n",
    "        \n",
    "        \n",
    "        train_loss = train_epoch(net, train_loader, loss_fn, optimizer)\n",
    "        test_loss = 0\n",
    "        scheduler.step()\n",
    "        print(\"Saving model....\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            test_loss, testmet = test_epoch(net, test_loader, loss_fn,use_net=True)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        #print(\"skipping testing cuz peak autism idk\")\n",
    "        \n",
    "        with open(basepath + \"/results.txt\",\"a\") as f:\n",
    "            f.write(\"Epoch :\"+str(e+1) + \"\\n\" + str(testmet))\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        print(\"OPed to txt\")\n",
    "        \n",
    "        torch.save(net.state_dict(), basepath +'/Weights/dc20_model_'+str(e+1)+'.pth')\n",
    "        torch.save(optimizer.state_dict(), basepath+'/Weights/dc20_opt_'+str(e+1)+'.pth')\n",
    "        \n",
    "        print(\"Models saved\")\n",
    "\n",
    "        # clear cache\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        #print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "        #              \"Loss: {:.6f}...\".format(train_loss),\n",
    "        #              \"Test Loss: {:.6f}\".format(test_loss))\n",
    "    return train_loss, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HO3p2zrOcn_z"
   },
   "source": [
    "### 20 Layer DCUNet Model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:35.020512Z",
     "iopub.status.busy": "2025-08-18T14:45:35.020354Z",
     "iopub.status.idle": "2025-08-18T14:45:35.041963Z",
     "shell.execute_reply": "2025-08-18T14:45:35.041276Z",
     "shell.execute_reply.started": "2025-08-18T14:45:35.020499Z"
    },
    "id": "j8XCrVIg5i-K",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DCUnet20(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Complex U-Net class of the model.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_fft=64, hop_length=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        # for istft\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        \n",
    "        self.set_size(model_complexity=int(45//1.414), input_channels=1, model_depth=20)\n",
    "        self.encoders = []\n",
    "        self.model_length = 20 // 2\n",
    "        \n",
    "        for i in range(self.model_length):\n",
    "            module = Encoder(in_channels=self.enc_channels[i], out_channels=self.enc_channels[i + 1],\n",
    "                             filter_size=self.enc_kernel_sizes[i], stride_size=self.enc_strides[i], padding=self.enc_paddings[i])\n",
    "            self.add_module(\"encoder{}\".format(i), module)\n",
    "            self.encoders.append(module)\n",
    "\n",
    "        self.decoders = []\n",
    "\n",
    "        for i in range(self.model_length):\n",
    "            if i != self.model_length - 1:\n",
    "                module = Decoder(in_channels=self.dec_channels[i] + self.enc_channels[self.model_length - i], out_channels=self.dec_channels[i + 1], \n",
    "                                 filter_size=self.dec_kernel_sizes[i], stride_size=self.dec_strides[i], padding=self.dec_paddings[i],\n",
    "                                 output_padding=self.dec_output_padding[i])\n",
    "            else:\n",
    "                module = Decoder(in_channels=self.dec_channels[i] + self.enc_channels[self.model_length - i], out_channels=self.dec_channels[i + 1], \n",
    "                                 filter_size=self.dec_kernel_sizes[i], stride_size=self.dec_strides[i], padding=self.dec_paddings[i],\n",
    "                                 output_padding=self.dec_output_padding[i], last_layer=True)\n",
    "            self.add_module(\"decoder{}\".format(i), module)\n",
    "            self.decoders.append(module)\n",
    "       \n",
    "        \n",
    "    def forward(self, x, is_istft=True):\n",
    "        # AGGIUNGERE: Assicurarsi che tutti i tensori siano sullo stesso dispositivo\n",
    "        device = next(self.parameters()).device\n",
    "        x = x.to(device)\n",
    "        # print('x : ', x.shape)\n",
    "        orig_x = x\n",
    "        xs = []\n",
    "        for i, encoder in enumerate(self.encoders):\n",
    "            xs.append(x)\n",
    "            x = encoder(x)\n",
    "            # print('Encoder : ', x.shape)\n",
    "            \n",
    "        p = x\n",
    "        for i, decoder in enumerate(self.decoders):\n",
    "            p = decoder(p)\n",
    "            if i == self.model_length - 1:\n",
    "                break\n",
    "            # print('Decoder : ', p.shape)\n",
    "            p = torch.cat([p, xs[self.model_length - 1 - i]], dim=1)\n",
    "        \n",
    "        # u9 - the mask\n",
    "        \n",
    "        mask = p\n",
    "        \n",
    "        # print('mask : ', mask.shape)\n",
    "        \n",
    "        output = mask * orig_x\n",
    "        output = torch.squeeze(output, 1)\n",
    "\n",
    "\n",
    "        if is_istft:\n",
    "            # CONVERSIONE NECESSARIA: da formato [..., 2] a tensore complesso nativo\n",
    "            output_complex = torch.view_as_complex(output)\n",
    "            output = torch.istft(output_complex, n_fft=self.n_fft, hop_length=self.hop_length, normalized=True, onesided=True)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    \n",
    "    def set_size(self, model_complexity, model_depth=20, input_channels=1):\n",
    "\n",
    "        if model_depth == 20:\n",
    "            self.enc_channels = [input_channels,\n",
    "                                 model_complexity,\n",
    "                                 model_complexity,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 128]\n",
    "\n",
    "            self.enc_kernel_sizes = [(7, 1),\n",
    "                                     (1, 7),\n",
    "                                     (6, 4),\n",
    "                                     (7, 5),\n",
    "                                     (5, 3),\n",
    "                                     (5, 3),\n",
    "                                     (5, 3),\n",
    "                                     (5, 3),\n",
    "                                     (5, 3),\n",
    "                                     (5, 3)]\n",
    "\n",
    "            self.enc_strides = [(1, 1),\n",
    "                                (1, 1),\n",
    "                                (2, 2),\n",
    "                                (2, 1),\n",
    "                                (2, 2),\n",
    "                                (2, 1),\n",
    "                                (2, 2),\n",
    "                                (2, 1),\n",
    "                                (2, 2),\n",
    "                                (2, 1)]\n",
    "\n",
    "            self.enc_paddings = [(3, 0),\n",
    "                                 (0, 3),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0)]\n",
    "\n",
    "            self.dec_channels = [0,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity * 2,\n",
    "                                 model_complexity,\n",
    "                                 model_complexity,\n",
    "                                 1]\n",
    "\n",
    "            self.dec_kernel_sizes = [(6, 3), \n",
    "                                     (6, 3),\n",
    "                                     (6, 3),\n",
    "                                     (6, 4),\n",
    "                                     (6, 3),\n",
    "                                     (6, 4),\n",
    "                                     (8, 5),\n",
    "                                     (7, 5),\n",
    "                                     (1, 7),\n",
    "                                     (7, 1)]\n",
    "\n",
    "            self.dec_strides = [(2, 1), #\n",
    "                                (2, 2), #\n",
    "                                (2, 1), #\n",
    "                                (2, 2), #\n",
    "                                (2, 1), #\n",
    "                                (2, 2), #\n",
    "                                (2, 1), #\n",
    "                                (2, 2), #\n",
    "                                (1, 1),\n",
    "                                (1, 1)]\n",
    "\n",
    "            self.dec_paddings = [(0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 0),\n",
    "                                 (0, 3),\n",
    "                                 (3, 0)]\n",
    "            \n",
    "            self.dec_output_padding = [(0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0),\n",
    "                                       (0,0)]\n",
    "        else:\n",
    "            raise ValueError(\"Unknown model depth : {}\".format(model_depth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6bIE9iOj8pq"
   },
   "source": [
    "## Training New Model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:35.042907Z",
     "iopub.status.busy": "2025-08-18T14:45:35.042708Z",
     "iopub.status.idle": "2025-08-18T14:45:38.225456Z",
     "shell.execute_reply": "2025-08-18T14:45:38.224718Z",
     "shell.execute_reply.started": "2025-08-18T14:45:35.042891Z"
    },
    "id": "QyBc1awQkI-D",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # clear cache\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "dcunet20 = DCUnet20(N_FFT, HOP_LENGTH).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(dcunet20.parameters())\n",
    "loss_fn = wsdr_fn\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:38.226454Z",
     "iopub.status.busy": "2025-08-18T14:45:38.226176Z",
     "iopub.status.idle": "2025-08-18T14:45:38.229945Z",
     "shell.execute_reply": "2025-08-18T14:45:38.229181Z",
     "shell.execute_reply.started": "2025-08-18T14:45:38.226438Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# specify paths and uncomment to resume training from a given point\n",
    "# model_checkpoint = torch.load(path_to_model)\n",
    "# opt_checkpoint = torch.load(path_to_opt)\n",
    "# dcunet20.load_state_dict(model_checkpoint)\n",
    "# optimizer.load_state_dict(opt_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:38.230897Z",
     "iopub.status.busy": "2025-08-18T14:45:38.230643Z",
     "iopub.status.idle": "2025-08-18T14:45:38.251017Z",
     "shell.execute_reply": "2025-08-18T14:45:38.250323Z",
     "shell.execute_reply.started": "2025-08-18T14:45:38.230859Z"
    },
    "id": "ppXkJUsY55vI",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#train_losses, test_losses = train(dcunet20, train_loader, test_loader, loss_fn, optimizer, scheduler, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pretrained weights to run denoising inference ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select the model weight .pth file ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:38.251979Z",
     "iopub.status.busy": "2025-08-18T14:45:38.251770Z",
     "iopub.status.idle": "2025-08-18T14:45:38.624020Z",
     "shell.execute_reply": "2025-08-18T14:45:38.623423Z",
     "shell.execute_reply.started": "2025-08-18T14:45:38.251964Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_weights_path = \"/kaggle/input/pretrainedweights/Pretrained_Weights/Noise2Noise/1.pth\"\n",
    "\n",
    "dcunet20 = DCUnet20(N_FFT, HOP_LENGTH).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(dcunet20.parameters())\n",
    "\n",
    "checkpoint = torch.load(model_weights_path,\n",
    "                        map_location=torch.device('cpu')\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select the testing audio folders for inference ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:38.624845Z",
     "iopub.status.busy": "2025-08-18T14:45:38.624636Z",
     "iopub.status.idle": "2025-08-18T14:45:41.851855Z",
     "shell.execute_reply": "2025-08-18T14:45:41.851055Z",
     "shell.execute_reply.started": "2025-08-18T14:45:38.624829Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_noisy_files = sorted(list(Path(\"/kaggle/input/white_noise_def/filtered_noise2noise_db_white_noise/test/input\").rglob('*.wav')))\n",
    "test_clean_files = sorted(list(Path(\"/kaggle/input/white_noise_def/filtered_noise2noise_db_white_noise/test/target\").rglob('*.wav')))\n",
    "\n",
    "test_dataset = SpeechDataset(test_noisy_files, test_clean_files, N_FFT, HOP_LENGTH)\n",
    "\n",
    "# For testing purpose\n",
    "test_loader_single_unshuffled = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:41.853028Z",
     "iopub.status.busy": "2025-08-18T14:45:41.852831Z",
     "iopub.status.idle": "2025-08-18T14:45:41.873236Z",
     "shell.execute_reply": "2025-08-18T14:45:41.872703Z",
     "shell.execute_reply.started": "2025-08-18T14:45:41.853013Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dcunet20.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enter the index of the file in the Test Set folder to Denoise and evaluate metrics waveforms (Indexing starts from 0) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:41.874140Z",
     "iopub.status.busy": "2025-08-18T14:45:41.873902Z",
     "iopub.status.idle": "2025-08-18T14:45:41.877385Z",
     "shell.execute_reply": "2025-08-18T14:45:41.876880Z",
     "shell.execute_reply.started": "2025-08-18T14:45:41.874117Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "index = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:41.878324Z",
     "iopub.status.busy": "2025-08-18T14:45:41.878104Z",
     "iopub.status.idle": "2025-08-18T14:45:43.854331Z",
     "shell.execute_reply": "2025-08-18T14:45:43.853574Z",
     "shell.execute_reply.started": "2025-08-18T14:45:41.878304Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dcunet20.eval()\n",
    "test_loader_single_unshuffled_iter = iter(test_loader_single_unshuffled)\n",
    "\n",
    "x_n, x_c = next(test_loader_single_unshuffled_iter)\n",
    "for _ in range(index):\n",
    "    x_n, x_c = next(test_loader_single_unshuffled_iter)\n",
    "\n",
    "x_est = dcunet20(x_n, is_istft=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:43.855167Z",
     "iopub.status.busy": "2025-08-18T14:45:43.854991Z",
     "iopub.status.idle": "2025-08-18T14:45:43.859376Z",
     "shell.execute_reply": "2025-08-18T14:45:43.858638Z",
     "shell.execute_reply.started": "2025-08-18T14:45:43.855153Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def safe_istft(stft_tensor, n_fft, hop_length, normalized=True):\n",
    "    \"\"\"\n",
    "    Sicura chiamata a torch.istft che gestisce automaticamente la conversione da real/imag a complesso\n",
    "    \"\"\"\n",
    "    if stft_tensor.shape[-1] == 2 and not torch.is_complex(stft_tensor):\n",
    "        # Converte da formato (real, imag) a complesso\n",
    "        stft_tensor = torch.view_as_complex(stft_tensor)\n",
    "    \n",
    "    return torch.istft(stft_tensor, n_fft=n_fft, hop_length=hop_length, normalized=normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:43.860971Z",
     "iopub.status.busy": "2025-08-18T14:45:43.860218Z",
     "iopub.status.idle": "2025-08-18T14:45:43.942275Z",
     "shell.execute_reply": "2025-08-18T14:45:43.941687Z",
     "shell.execute_reply.started": "2025-08-18T14:45:43.860948Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "x_est_np = x_est[0].view(-1).detach().cpu().numpy()\n",
    "x_c_np = safe_istft(torch.squeeze(x_c, 1), N_FFT, HOP_LENGTH).view(-1).detach().cpu().numpy()\n",
    "x_n_np = safe_istft(torch.squeeze(x_n, 1), N_FFT, HOP_LENGTH).view(-1).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:43.945964Z",
     "iopub.status.busy": "2025-08-18T14:45:43.945759Z",
     "iopub.status.idle": "2025-08-18T14:45:43.950340Z",
     "shell.execute_reply": "2025-08-18T14:45:43.949762Z",
     "shell.execute_reply.started": "2025-08-18T14:45:43.945949Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === SNR Function ===\n",
    "def compute_snr(clean, estimate):\n",
    "    # Converti in tensori se sono array numpy\n",
    "    if isinstance(clean, np.ndarray):\n",
    "        clean = torch.tensor(clean, dtype=torch.float32)\n",
    "    if isinstance(estimate, np.ndarray):\n",
    "        estimate = torch.tensor(estimate, dtype=torch.float32)\n",
    "    \n",
    "    signal_power = torch.sum(clean ** 2)\n",
    "    noise_power = torch.sum((clean - estimate) ** 2)\n",
    "    \n",
    "    # Evita log(0) con epsilon più robusto\n",
    "    eps = torch.finfo(clean.dtype).eps\n",
    "    snr = 10 * torch.log10(signal_power / (noise_power + eps))\n",
    "    return snr.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of denoising the audio in /Samples folder ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noisy audio waveform ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:43.951318Z",
     "iopub.status.busy": "2025-08-18T14:45:43.951109Z",
     "iopub.status.idle": "2025-08-18T14:45:44.777118Z",
     "shell.execute_reply": "2025-08-18T14:45:44.776326Z",
     "shell.execute_reply.started": "2025-08-18T14:45:43.951302Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.plot(x_n_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model denoised audio waveform ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:44.778624Z",
     "iopub.status.busy": "2025-08-18T14:45:44.777970Z",
     "iopub.status.idle": "2025-08-18T14:45:45.100515Z",
     "shell.execute_reply": "2025-08-18T14:45:45.099810Z",
     "shell.execute_reply.started": "2025-08-18T14:45:44.778599Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.plot(x_est_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### True clean audio waveform ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:45.101536Z",
     "iopub.status.busy": "2025-08-18T14:45:45.101293Z",
     "iopub.status.idle": "2025-08-18T14:45:45.435251Z",
     "shell.execute_reply": "2025-08-18T14:45:45.434508Z",
     "shell.execute_reply.started": "2025-08-18T14:45:45.101514Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.plot(x_c_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Recently Denoised Speech Files ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:45:45.436217Z",
     "iopub.status.busy": "2025-08-18T14:45:45.436030Z",
     "iopub.status.idle": "2025-08-18T14:45:45.465623Z",
     "shell.execute_reply": "2025-08-18T14:45:45.465117Z",
     "shell.execute_reply.started": "2025-08-18T14:45:45.436202Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "# Salva direttamente gli array numpy\n",
    "sf.write(\"/kaggle/working/predicted.wav\", x_est_np, SAMPLE_RATE)\n",
    "sf.write(\"/kaggle/working/clean.wav\", x_c_np, SAMPLE_RATE) \n",
    "sf.write(\"/kaggle/working/noisy.wav\", x_n_np, SAMPLE_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T16:02:34.043116Z",
     "iopub.status.busy": "2025-08-18T16:02:34.042820Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# Device e parametri globali\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class AudioProcessor:\n",
    "    \"\"\"Classe per gestire elaborazione audio\"\"\"\n",
    "    \n",
    "    def __init__(self, n_fft, hop_length, sample_rate):\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.sample_rate = sample_rate\n",
    "        self.window = torch.hann_window(n_fft).to(device)\n",
    "\n",
    "    def stft_to_waveform(self, stft_tensor):\n",
    "        \"\"\"Conversione STFT->waveform\"\"\"\n",
    "        if stft_tensor.dim() == 5:\n",
    "            stft_tensor = stft_tensor[0, 0]\n",
    "        elif stft_tensor.dim() == 4:\n",
    "            stft_tensor = stft_tensor[0]\n",
    "        \n",
    "        complex_tensor = torch.view_as_complex(stft_tensor)\n",
    "        waveform = torch.istft(\n",
    "            complex_tensor,\n",
    "            n_fft=self.n_fft,\n",
    "            hop_length=self.hop_length,\n",
    "            window=self.window,\n",
    "            normalized=True,\n",
    "            return_complex=False\n",
    "        )\n",
    "        return waveform\n",
    "\n",
    "    def ensure_waveform(self, tensor):\n",
    "        \"\"\"Converte tensor a waveform\"\"\"\n",
    "        if tensor.dim() >= 3 and tensor.size(-1) == 2:\n",
    "            return self.stft_to_waveform(tensor)\n",
    "        else:\n",
    "            while tensor.dim() > 1 and tensor.size(0) == 1:\n",
    "                tensor = tensor.squeeze(0)\n",
    "            return tensor\n",
    "\n",
    "class SingleSamplePlotter:\n",
    "    def __init__(self, sample_rate, save_dir='/kaggle/working'):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.save_dir = Path(save_dir)\n",
    "        self.save_dir.mkdir(exist_ok=True)\n",
    "    def plot(self, clean_wave, noisy_wave, predicted_wave, sample_idx, snr_value, snr_improvement):\n",
    "        def to_numpy(tensor):\n",
    "            if torch.is_tensor(tensor):\n",
    "                return tensor.detach().cpu().numpy().squeeze()\n",
    "            return np.asarray(tensor).squeeze()\n",
    "        try:\n",
    "            clean_np, noisy_np, pred_np = map(to_numpy, [clean_wave, noisy_wave, predicted_wave])\n",
    "            min_len = min(len(clean_np), len(noisy_np), len(pred_np))\n",
    "            clean_np = clean_np[:min_len]\n",
    "            noisy_np = noisy_np[:min_len]\n",
    "            pred_np = pred_np[:min_len]\n",
    "            time_axis = np.arange(min_len) / self.sample_rate\n",
    "            fig, axes = plt.subplots(3, 1, figsize=(12, 8))\n",
    "            colors = ['#2E8B57', '#DC143C', '#1E90FF']\n",
    "            signals = [('Clean (Ref)', clean_np), ('Noisy (Input)', noisy_np), ('Denoised (Output)', pred_np)]\n",
    "            for i, (label, signal) in enumerate(signals):\n",
    "                axes[i].plot(time_axis, signal, color=colors[i], linewidth=0.8, alpha=0.9)\n",
    "                if i == 2:\n",
    "                    title = f'{label} - SNR: {snr_value:.2f} dB | ΔSNR: {snr_improvement:+.2f} dB'\n",
    "                else:\n",
    "                    title = f'Sample {sample_idx} - {label}'\n",
    "                axes[i].set_title(title, fontsize=11)\n",
    "                axes[i].set_ylabel('Amplitude')\n",
    "                axes[i].grid(True, alpha=0.3)\n",
    "            axes[-1].set_xlabel('Time (seconds)')\n",
    "            plt.tight_layout()\n",
    "            filename = self.save_dir / f'sample_{sample_idx:03d}_plot.png'\n",
    "            plt.savefig(filename, dpi=120, bbox_inches='tight', facecolor='white')\n",
    "            plt.close()\n",
    "            return str(filename)\n",
    "        except Exception as e:\n",
    "            print(f\" Errore plot campione {sample_idx}: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "class SNRCalculator:\n",
    "    \"\"\"Classe per calcoli SNR\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_snr(clean, estimate, epsilon=1e-10):\n",
    "        signal_power = torch.sum(clean ** 2)\n",
    "        noise_power = torch.sum((clean - estimate) ** 2)\n",
    "        noise_power = torch.clamp(noise_power, min=epsilon)\n",
    "        snr = 10 * torch.log10(signal_power / noise_power)\n",
    "        return snr.item()\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_ssnr(clean, estimate, frame_length=1440, frame_shift=720, C_min=-10, C_max=35):\n",
    "        clean = clean.squeeze().cpu().numpy()\n",
    "        estimate = estimate.squeeze().cpu().numpy()\n",
    "        min_len = min(len(clean), len(estimate))\n",
    "        clean = clean[:min_len]\n",
    "        estimate = estimate[:min_len]\n",
    "        n_frames = (min_len - frame_length) // frame_shift + 1\n",
    "        if n_frames < 1:\n",
    "            return float('nan')\n",
    "        ssnr_list = []\n",
    "        for i in range(n_frames):\n",
    "            start = i * frame_shift\n",
    "            end = start + frame_length\n",
    "            c_seg = clean[start:end]\n",
    "            e_seg = estimate[start:end]\n",
    "            num = np.sum(c_seg ** 2)\n",
    "            den = np.sum((c_seg - e_seg) ** 2)\n",
    "            # Proteggi il rapporto\n",
    "            ratio = num / (den + 1e-8)\n",
    "            if ratio <= 0 or not np.isfinite(ratio):\n",
    "                snr = C_min\n",
    "            else:\n",
    "                snr = 10 * np.log10(ratio)\n",
    "            snr = np.clip(snr, C_min, C_max)\n",
    "            ssnr_list.append(snr)\n",
    "        return np.mean(ssnr_list)\n",
    "\n",
    "    @staticmethod\n",
    "    def align_tensors(*tensors):\n",
    "        min_len = min(t.shape[-1] for t in tensors)\n",
    "        return [t[..., :min_len] for t in tensors]\n",
    "\n",
    "def save_audio_sample(clean_wave, noisy_wave, predicted_wave, sample_idx, \n",
    "                     sample_rate, save_dir='/kaggle/working'):\n",
    "    \"\"\"Salva i tre audio (clean, noisy, denoised) per un campione specifico\"\"\"\n",
    "    \n",
    "    save_path = Path(save_dir)\n",
    "    save_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    def to_cpu_tensor(tensor):\n",
    "        if torch.is_tensor(tensor):\n",
    "            return tensor.detach().cpu().unsqueeze(0) if tensor.dim() == 1 else tensor.detach().cpu()\n",
    "        return torch.tensor(tensor).unsqueeze(0)\n",
    "    \n",
    "    # Converti a tensori CPU\n",
    "    clean_cpu = to_cpu_tensor(clean_wave)\n",
    "    noisy_cpu = to_cpu_tensor(noisy_wave)\n",
    "    predicted_cpu = to_cpu_tensor(predicted_wave)\n",
    "    \n",
    "    # Salva i file audio\n",
    "    files_saved = []\n",
    "    \n",
    "    try:\n",
    "        clean_file = save_path / f'sample_{sample_idx:03d}_clean.wav'\n",
    "        torchaudio.save(str(clean_file), clean_cpu, sample_rate)\n",
    "        files_saved.append(str(clean_file))\n",
    "        \n",
    "        noisy_file = save_path / f'sample_{sample_idx:03d}_noisy.wav'\n",
    "        torchaudio.save(str(noisy_file), noisy_cpu, sample_rate)\n",
    "        files_saved.append(str(noisy_file))\n",
    "        \n",
    "        denoised_file = save_path / f'sample_{sample_idx:03d}_denoised.wav'\n",
    "        torchaudio.save(str(denoised_file), predicted_cpu, sample_rate)\n",
    "        files_saved.append(str(denoised_file))\n",
    "        \n",
    "        print(f\" Audio salvati per campione {sample_idx}:\")\n",
    "        for file in files_saved:\n",
    "            print(f\"   • {Path(file).name}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\" Errore salvataggio audio campione {sample_idx}: {e}\")\n",
    "    \n",
    "    return files_saved\n",
    "\n",
    "def evaluate_testset(model, test_loader, audio_processor, snr_calculator, \n",
    "                    sample_rate, save_audio_for=None, plot_samples=None, plotter=None):\n",
    "    \"\"\"Valutazione del test set con opzione salvataggio audio\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    snr_values = []\n",
    "    snr_improvements = []\n",
    "    ssnr_values = []\n",
    "    ssnr_improvements = []\n",
    "    \n",
    "    print(\" Avvio valutazione test set...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (noisy_input, clean_input) in enumerate(tqdm(test_loader, desc=\"Processing\")):\n",
    "            try:\n",
    "                noisy_input = noisy_input.to(device, non_blocking=True)\n",
    "                clean_input = clean_input.to(device, non_blocking=True)\n",
    "                \n",
    "                # Predizione\n",
    "                predicted_output = model(noisy_input, is_istft=True)\n",
    "                \n",
    "                # Conversione a waveform\n",
    "                predicted_wave = audio_processor.ensure_waveform(predicted_output)\n",
    "                clean_wave = audio_processor.ensure_waveform(clean_input)\n",
    "                noisy_wave = audio_processor.ensure_waveform(noisy_input)\n",
    "                \n",
    "                # Allineamento\n",
    "                clean_aligned, predicted_aligned, noisy_aligned = snr_calculator.align_tensors(\n",
    "                    clean_wave, predicted_wave, noisy_wave)\n",
    "                \n",
    "                # Calcolo SNR\n",
    "                snr_pred = snr_calculator.compute_snr(clean_aligned, predicted_aligned)\n",
    "                snr_noisy = snr_calculator.compute_snr(clean_aligned, noisy_aligned)\n",
    "                snr_improvement = snr_pred - snr_noisy\n",
    "\n",
    "                # SSNR e miglioramento\n",
    "                ssnr_pred = snr_calculator.compute_ssnr(clean_aligned, predicted_aligned)\n",
    "                ssnr_noisy = snr_calculator.compute_ssnr(clean_aligned, noisy_aligned)\n",
    "                ssnr_improvement = ssnr_pred - ssnr_noisy\n",
    "                \n",
    "                snr_values.append(snr_pred)\n",
    "                snr_improvements.append(snr_improvement)\n",
    "                ssnr_values.append(ssnr_pred)\n",
    "                ssnr_improvements.append(ssnr_improvement)\n",
    "                \n",
    "                # Salva audio se richiesto per questo campione\n",
    "                if save_audio_for and (i + 1) in save_audio_for:\n",
    "                    save_audio_sample(clean_aligned, noisy_aligned, predicted_aligned, \n",
    "                                    i + 1, sample_rate)\n",
    "                # Salva plot se richiesto\n",
    "                if plotter and plot_samples and (i + 1) in plot_samples:\n",
    "                    plot_path = plotter.plot(clean_aligned, noisy_aligned, predicted_aligned, i + 1, snr_pred, snr_improvement)\n",
    "                    print(f\" Plot salvato per sample {i+1}: {plot_path}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\" Errore campione {i+1}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return snr_values, snr_improvements, ssnr_values, ssnr_improvements\n",
    "\n",
    "def plot_results(snr_values, snr_improvements, save_dir='/kaggle/working'):\n",
    "    \"\"\"Crea i due istogrammi delle distribuzioni\"\"\"\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Distribuzione SNR\n",
    "    ax1.hist(snr_values, bins=40, alpha=0.7, color='blue', edgecolor='navy')\n",
    "    mean_snr = np.mean(snr_values)\n",
    "    # ax1.set_xlim(0, 30) PER SETTARE DIMENSIONI FISSE PER L'ASCISSA\n",
    "    ax1.axvline(mean_snr, color='red', linestyle='--', linewidth=2,\n",
    "               label=f'Media: {mean_snr:.2f} dB')\n",
    "    ax1.set_title('Distribuzione SNR Predicted vs Clean')\n",
    "    ax1.set_xlabel('SNR (dB)')\n",
    "    ax1.set_ylabel('Frequenza')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Distribuzione miglioramenti\n",
    "    ax2.hist(snr_improvements, bins=40, alpha=0.7, color='green', edgecolor='darkgreen')\n",
    "    mean_imp = np.mean(snr_improvements)\n",
    "    ax2.axvline(mean_imp, color='red', linestyle='--', linewidth=2,\n",
    "               label=f'Media: {mean_imp:.2f} dB')\n",
    "    ax2.axvline(0, color='black', linestyle='-', alpha=0.5, label='No improvement')\n",
    "    ax2.set_title('Distribuzione SNR Improvements')\n",
    "    ax2.set_xlabel('SNR Improvement (dB)')\n",
    "    ax2.set_ylabel('Frequenza')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Salva il grafico\n",
    "    save_path = Path(save_dir)\n",
    "    save_path.mkdir(exist_ok=True)\n",
    "    plot_file = save_path / 'snr_distributions.png'\n",
    "    plt.savefig(plot_file, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return str(plot_file)\n",
    "\n",
    "def main(save_audio_samples=None, plot_samples=None):\n",
    "    \"\"\"Funzione principale - specifica save_audio_samples=[1,5,10] per salvare audio\"\"\"\n",
    "    \n",
    "    # Inizializza componenti\n",
    "    audio_processor = AudioProcessor(N_FFT, HOP_LENGTH, SAMPLE_RATE)\n",
    "    plotter = SingleSamplePlotter(SAMPLE_RATE)\n",
    "    snr_calculator = SNRCalculator()\n",
    "    \n",
    "    # Carica modello\n",
    "    print(\" Caricamento modello...\")\n",
    "    model = DCUnet20(n_fft=N_FFT, hop_length=HOP_LENGTH).to(device)\n",
    "    model.load_state_dict(torch.load(\"/kaggle/input/pretrainedweights/Pretrained_Weights/Noise2Noise/white.pth\", \n",
    "                                   map_location=device))\n",
    "    \n",
    "    print(f\" Setup completato - Device: {device}\")\n",
    "    \n",
    "    # Valutazione\n",
    "    snr_values, snr_improvements, ssnr_values, ssnr_improvements = evaluate_testset(\n",
    "        model=model,\n",
    "        test_loader=test_loader_single_unshuffled,\n",
    "        audio_processor=audio_processor,\n",
    "        snr_calculator=snr_calculator,\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        save_audio_for=save_audio_samples,\n",
    "        plot_samples=plot_samples,\n",
    "        plotter=plotter\n",
    "    )\n",
    "    \n",
    "    # Stampa risultati come nell'immagine\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" RISULTATI FINALI DEL TEST SET\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Totale campioni processati: {len(snr_values)}\")\n",
    "    print(f\"SNR medio predicted vs clean: {np.mean(snr_values):.2f} ± {np.std(snr_values):.2f} dB\")\n",
    "    print(f\"SNR improvement medio: {np.mean(snr_improvements):.2f} ± {np.std(snr_improvements):.2f} dB\")\n",
    "    print(f\"SNR minimo: {np.min(snr_values):.2f} dB\")\n",
    "    print(f\"SNR massimo: {np.max(snr_values):.2f} dB\")\n",
    "    print(f\"Mediana SNR: {np.median(snr_values):.2f} dB\")\n",
    "\n",
    "    print(f\"SSNR medio predicted vs clean: {np.mean(ssnr_values):.2f} ± {np.std(ssnr_values):.2f} dB\")\n",
    "    print(f\"SSNR improvement medio: {np.mean(ssnr_improvements):.2f} ± {np.std(ssnr_improvements):.2f} dB\")\n",
    "\n",
    "    \n",
    "    improvement_rate = (sum(1 for x in snr_improvements if x > 0) / len(snr_improvements)) * 100\n",
    "    print(f\"Campioni con miglioramento: {sum(1 for x in snr_improvements if x > 0)}/{len(snr_improvements)} ({improvement_rate:.1f}%)\")\n",
    "    \n",
    "    # Crea grafici\n",
    "    plot_file = plot_results(snr_values, snr_improvements)\n",
    "    print(f\"\\n💾 Grafico salvato: {plot_file}\")\n",
    "    \n",
    "    return snr_values, snr_improvements\n",
    "\n",
    "# ESECUZIONE\n",
    "# Per salvare audio di campioni specifici, usa:\n",
    "# results = main(save_audio_samples=[1, 5, 10])  # salva audio dei campioni 1, 5 e 10\n",
    "# \n",
    "# Per non salvare audio:\n",
    "# results = main()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Cambia qui i numeri dei campioni di cui vuoi salvare l'audio\n",
    "    SAMPLES_TO_SAVE_AND_PLOT = [1, 3, 10]  # Modifica con i numeri che preferisci\n",
    "    \n",
    "    print(\" Avvio valutazione...\")\n",
    "    results = main(save_audio_samples=SAMPLES_TO_SAVE_AND_PLOT, plot_samples=SAMPLES_TO_SAVE_AND_PLOT)\n",
    "    print(\" Valutazione completata!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "dcunet.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7846055,
     "sourceId": 12438434,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7846117,
     "sourceId": 12438520,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7846568,
     "sourceId": 12439163,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8103173,
     "sourceId": 12814727,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8119038,
     "sourceId": 12837364,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8119257,
     "sourceId": 12837686,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
