{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Frads01/NoiseRemover/blob/main/NoiseRemover.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Noise Remover\n"
      ],
      "metadata": {
        "id": "OYfLqR29sPnz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import librerie:"
      ],
      "metadata": {
        "id": "RisdxjMgEB7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "base_dir = Path(\"/content/drive/dataset/\")\n",
        "(base_dir / \"Weights\").mkdir(parents=True, exist_ok=True)\n",
        "(base_dir / \"Samples\").mkdir(parents=True, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "n4b5v3TgYi0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gc\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "\n",
        "# determinismo CUDA GPU\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n"
      ],
      "metadata": {
        "id": "yycpjxF9EEkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indicazione Path per i dati di input e target:"
      ],
      "metadata": {
        "id": "Tvl_FjKEC-6F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JG7bdC-hsKtv"
      },
      "outputs": [],
      "source": [
        "TRAIN_INPUT_DIR = base_dir / 'train' / 'input'\n",
        "TRAIN_TARGET_DIR = base_dir / 'train'/ 'target'\n",
        "\n",
        "TEST_NOISY_DIR = base_dir / 'test' / 'input'\n",
        "TEST_CLEAN_DIR = base_dir / 'test'/ 'clean'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parametri per la trasformazione STFT:"
      ],
      "metadata": {
        "id": "46TSDkqFEuKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SAMPLE_RATE = 44100\n",
        "n_fft = 2048 # grandezza della finestra (risoluzione in frequenza) - nel paper è 3072, ottima per il parlato\n",
        "hop_length = 512 # salto tra una finestra e l’altra (risoluzione temporale) - nel paper è 768"
      ],
      "metadata": {
        "id": "BdHJ8JPvEzOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dichiarazioni del Dataset e del Dataloader"
      ],
      "metadata": {
        "id": "x8EeJlBLJm2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Noise2NoiseDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset per training Noise2Noise su audio.\n",
        "    Ogni esempio ha due versioni rumorose dello stesso contenuto.\n",
        "    \"\"\"\n",
        "    def __init__(self, noisy_file_set_A, noisy_file_set_B, n_fft=1024, hop_length=256):\n",
        "        super().__init__()\n",
        "        self.noisy_A = sorted(noisy_file_set_A)\n",
        "        self.noisy_B = sorted(noisy_file_set_B)\n",
        "\n",
        "        assert len(self.noisy_A) == len(self.noisy_B), \"Le due liste devono avere la stessa lunghezza.\"\n",
        "\n",
        "        self.n_fft = n_fft\n",
        "        self.hop_length = hop_length\n",
        "        self.max_len = 165000  # puoi regolarla\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.noisy_A)\n",
        "\n",
        "    def load_sample(self, file):\n",
        "        waveform, _ = torchaudio.load(file)\n",
        "        return waveform\n",
        "\n",
        "    def _prepare_sample(self, waveform):\n",
        "        \"\"\"Pad o tronca a self.max_len campioni, da sinistra o destra.\"\"\"\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = waveform[:1, :]  # prendi solo 1 canale\n",
        "\n",
        "        current_len = waveform.shape[1]\n",
        "\n",
        "        if current_len >= self.max_len:\n",
        "            waveform = waveform[:, :self.max_len]\n",
        "        else:\n",
        "            pad_len = self.max_len - current_len\n",
        "            waveform = torch.nn.functional.pad(waveform, (0, pad_len), mode='constant', value=0.0)\n",
        "\n",
        "        return waveform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      # carica audio\n",
        "        x1 = self.load_sample(self.noisy_A[index])\n",
        "        x2 = self.load_sample(self.noisy_B[index])\n",
        "\n",
        "        # padding/troncamento\n",
        "        x1 = self._prepare_sample(x1)\n",
        "        x2 = self._prepare_sample(x2)\n",
        "\n",
        "        # STFT\n",
        "        x1_stft = torch.stft(x1, n_fft=self.n_fft, hop_length=self.hop_length, normalized=True, return_complex=True)\n",
        "        x2_stft = torch.stft(x2, n_fft=self.n_fft, hop_length=self.hop_length, normalized=True, return_complex=True)\n",
        "\n",
        "        return x1_stft, x2_stft\n",
        "\n",
        "files_noise_input = sorted(list(TRAIN_INPUT_DIR.rglob(\"*.wav\")))\n",
        "files_noise_target = sorted(list(TRAIN_TARGET_DIR.rglob(\"*.wav\")))\n",
        "test_noisy_files = sorted(list(TEST_NOISY_DIR.rglob('*.wav')))\n",
        "test_clean_files = sorted(list(TEST_CLEAN_DIR.rglob('*.wav')))\n",
        "\n",
        "print(\"No. of Training files:\",len(files_noise_input))\n",
        "print(\"No. of Test files:\",len(test_noisy_files))\n",
        "\n",
        "noise2noise_dataset = Noise2NoiseDataset(files_noise_input, files_noise_target, n_fft, hop_length)\n",
        "test_dataset = Noise2NoiseDataset(test_noisy_files, test_clean_files, n_fft, hop_length)\n",
        "\n",
        "train_loader = DataLoader(noise2noise_dataset, batch_size=2, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# For testing purpose\n",
        "#test_loader_single_unshuffled = DataLoader(test_dataset, batch_size=1, shuffle=False)"
      ],
      "metadata": {
        "id": "QYHVs_vnJrGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metriche"
      ],
      "metadata": {
        "id": "XIggG8HOJ27m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioMetricsGeneral:\n",
        "    def __init__(self, target_audio, input_audio, fs):\n",
        "        if len(target_audio) != len(input_audio):\n",
        "            raise Exception(\"Signal lengths don't match!\")\n",
        "\n",
        "        self.min_cutoff = 0.01\n",
        "        self.clip_values = (-self.min_cutoff, self.min_cutoff)\n",
        "\n",
        "        clean_audio = np.zeros(shape=target_audio.shape)\n",
        "        processed_audio = np.zeros(shape=input_audio.shape)\n",
        "\n",
        "        for index, data in np.ndenumerate(target_audio):\n",
        "            if data == 0:\n",
        "                clean_audio[index] = 0.01\n",
        "            else:\n",
        "                clean_audio[index] = data\n",
        "\n",
        "        for index, data in np.ndenumerate(input_audio):\n",
        "            if data == 0:\n",
        "                processed_audio[index] = 0.01\n",
        "            else:\n",
        "                processed_audio[index] = data\n",
        "\n",
        "        self.SNR = snr(target_audio, input_audio)\n",
        "        self.SSNR = SNRseg(target_audio, input_audio, fs)\n",
        "\n",
        "    def display(self):\n",
        "        fstring = \"{} : {:.3f}\"\n",
        "        metric_names = [\"SNR\", \"SSNR\"]\n",
        "        for name in metric_names:\n",
        "            metric_value = eval(\"self.\" + name)\n",
        "            print(fstring.format(name, metric_value))\n",
        "\n",
        "\n",
        "# Formula Reference: http://www.irisa.fr/armor/lesmembres/Mohamed/Thesis/node94.html\n",
        "\n",
        "def snr(reference, test):\n",
        "    numerator = 0.0\n",
        "    denominator = 0.0\n",
        "    for i in range(len(reference)):\n",
        "        numerator += reference[i]**2\n",
        "        denominator += (reference[i] - test[i])**2\n",
        "    return 10*np.log10(numerator/denominator)\n",
        "\n",
        "\n",
        "# Reference : https://github.com/schmiph2/pysepm\n",
        "\n",
        "def SNRseg(clean_signal, processed_signal, fs, frameLen=0.1, overlap=0.5):\n",
        "    eps = np.finfo(np.float64).eps\n",
        "\n",
        "    winlength = round(frameLen * fs)  # finestra in campioni (es. 0.1 s * fs)\n",
        "    skiprate = int(np.floor((1 - overlap) * winlength))  # salto tra finestre\n",
        "\n",
        "    MIN_SNR = -10  # minimo SNR in dB\n",
        "    MAX_SNR = 35   # massimo SNR in dB\n",
        "\n",
        "    # finestra di Hann\n",
        "    hannWin = 0.5 * (1 - np.cos(2 * np.pi * np.arange(1, winlength + 1) / (winlength + 1)))\n",
        "\n",
        "    # suddividi i segnali in finestre sovrapposte\n",
        "    clean_framed = extract_overlapped_windows(clean_signal, winlength, winlength - skiprate, hannWin)\n",
        "    processed_framed = extract_overlapped_windows(processed_signal, winlength, winlength - skiprate, hannWin)\n",
        "\n",
        "    # calcola energia del segnale e del rumore per ogni finestra\n",
        "    signal_energy = np.power(clean_framed, 2).sum(-1)\n",
        "    noise_energy = np.power(clean_framed - processed_framed, 2).sum(-1)\n",
        "\n",
        "    # calcola SNR segmentale per ogni finestra\n",
        "    segmental_snr = 10 * np.log10(signal_energy / (noise_energy + eps) + eps)\n",
        "\n",
        "    # limita i valori entro intervallo ragionevole\n",
        "    segmental_snr[segmental_snr < MIN_SNR] = MIN_SNR\n",
        "    segmental_snr[segmental_snr > MAX_SNR] = MAX_SNR\n",
        "\n",
        "    segmental_snr = segmental_snr[:-1]  # rimuovi ultima finestra, non valida\n",
        "\n",
        "    return np.mean(segmental_snr)\n",
        "\n",
        "def extract_overlapped_windows(x,nperseg,noverlap,window=None):\n",
        "    step = nperseg - noverlap\n",
        "    shape = x.shape[:-1]+((x.shape[-1]-noverlap)//step, nperseg)\n",
        "    strides = x.strides[:-1]+(step*x.strides[-1], x.strides[-1])\n",
        "    result = np.lib.stride_tricks.as_strided(x, shape=shape,\n",
        "                                             strides=strides)\n",
        "    if window is not None:\n",
        "        result = window * result\n",
        "    return result"
      ],
      "metadata": {
        "id": "dszRut0vJ6PH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definizione dei diversi layer"
      ],
      "metadata": {
        "id": "SHS5BTBuBTtz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layer convoluzionale per segnali complessi:"
      ],
      "metadata": {
        "id": "WWvdTgRc9NKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ComplexConv2d(nn.Module):  # convoluzione 2D su numeri complessi\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
        "      super().__init__()\n",
        "\n",
        "      self.in_channels = in_channels\n",
        "      self.out_channels = out_channels\n",
        "      self.kernel_size = kernel_size\n",
        "      self.padding = padding\n",
        "      self.stride = stride\n",
        "\n",
        "      # crea una convoluzione per la parte reale:\n",
        "      self.real_conv = nn.Conv2d(in_channels=self.in_channels,\n",
        "                                 out_channels=self.out_channels,\n",
        "                                 kernel_size=self.kernel_size,\n",
        "                                 padding=self.padding,\n",
        "                                 stride=self.stride)\n",
        "\n",
        "      # crea un’altra convoluzione per la parte immaginaria.\n",
        "      # Nota: è separata, quindi ha i suoi pesi e bias distinti.\n",
        "      self.im_conv = nn.Conv2d(in_channels=self.in_channels,\n",
        "                               out_channels=self.out_channels,\n",
        "                               kernel_size=self.kernel_size,\n",
        "                               padding=self.padding,\n",
        "                               stride=self.stride)\n",
        "\n",
        "      # Glorot initialization.\n",
        "      nn.init.xavier_uniform_(self.real_conv.weight)\n",
        "      nn.init.xavier_uniform_(self.im_conv.weight)\n",
        "\n",
        "  def forward(self, x):  # x: è un tensore che contiene, sull’ultima dimensione, la parte reale e immaginaria\n",
        "        x_real = x[..., 0]\n",
        "        x_im = x[..., 1]\n",
        "\n",
        "        # calcolo convoluzione complessa\n",
        "        c_real = self.real_conv(x_real) - self.im_conv(x_im)\n",
        "        c_im = self.im_conv(x_real) + self.real_conv(x_im)\n",
        "\n",
        "        # combino le due parti (reale e immaginaria) di nuovo insieme, lungo l’ultima dimensione (dim = -1), per restituire un tensore complesso.\n",
        "        output = torch.stack([c_real, c_im], dim=-1)\n",
        "        return output"
      ],
      "metadata": {
        "id": "2QgFF95s9XM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layer per deconvoluzione di segnali complessi:"
      ],
      "metadata": {
        "id": "sPG_oG9ZAytW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ComplexConvTranspose2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, output_padding=0, padding=0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.output_padding = output_padding\n",
        "        self.padding = padding\n",
        "        self.stride = stride\n",
        "\n",
        "        self.real_convt = nn.ConvTranspose2d(in_channels=self.in_channels,\n",
        "                                            out_channels=self.out_channels,\n",
        "                                            kernel_size=self.kernel_size,\n",
        "                                            output_padding=self.output_padding,\n",
        "                                            padding=self.padding,\n",
        "                                            stride=self.stride)\n",
        "\n",
        "        self.im_convt = nn.ConvTranspose2d(in_channels=self.in_channels,\n",
        "                                            out_channels=self.out_channels,\n",
        "                                            kernel_size=self.kernel_size,\n",
        "                                            output_padding=self.output_padding,\n",
        "                                            padding=self.padding,\n",
        "                                            stride=self.stride)\n",
        "\n",
        "\n",
        "        # Glorot initialization.\n",
        "        nn.init.xavier_uniform_(self.real_convt.weight)\n",
        "        nn.init.xavier_uniform_(self.im_convt.weight)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_real = x[..., 0]\n",
        "        x_im = x[..., 1]\n",
        "\n",
        "        ct_real = self.real_convt(x_real) - self.im_convt(x_im)\n",
        "        ct_im = self.im_convt(x_real) + self.real_convt(x_im)\n",
        "\n",
        "        output = torch.stack([ct_real, ct_im], dim=-1)\n",
        "        return output"
      ],
      "metadata": {
        "id": "hwUoRLXXA5Op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layer per la batch normalization di segnali complessi:"
      ],
      "metadata": {
        "id": "q-UT9p42B8DV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ComplexBatchNorm2d(nn.Module):\n",
        "    def __init__(self, num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps\n",
        "        self.momentum = momentum\n",
        "        self.affine = affine\n",
        "        self.track_running_stats = track_running_stats\n",
        "\n",
        "        self.real_b = nn.BatchNorm2d(num_features=self.num_features, eps=self.eps, momentum=self.momentum,\n",
        "                                      affine=self.affine, track_running_stats=self.track_running_stats)\n",
        "        self.im_b = nn.BatchNorm2d(num_features=self.num_features, eps=self.eps, momentum=self.momentum,\n",
        "                                    affine=self.affine, track_running_stats=self.track_running_stats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_real = x[..., 0]\n",
        "        x_im = x[..., 1]\n",
        "\n",
        "        n_real = self.real_b(x_real)\n",
        "        n_im = self.im_b(x_im)\n",
        "\n",
        "        output = torch.stack([n_real, n_im], dim=-1)\n",
        "        return output"
      ],
      "metadata": {
        "id": "I3yANC1OCCIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layer Encoder:"
      ],
      "metadata": {
        "id": "2odkCO2UCoDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, filter_size=(7,5), stride_size=(2,2), in_channels=1, out_channels=45, padding=(0,0)):\n",
        "        super().__init__()\n",
        "\n",
        "        self.filter_size = filter_size\n",
        "        self.stride_size = stride_size\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.padding = padding\n",
        "\n",
        "        self.cconv = ComplexConv2d(in_channels=self.in_channels, out_channels=self.out_channels,\n",
        "                             kernel_size=self.filter_size, stride=self.stride_size, padding=self.padding)\n",
        "\n",
        "        self.cbn = ComplexBatchNorm2d(num_features=self.out_channels)\n",
        "\n",
        "        self.leaky_relu = nn.LeakyReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        conved = self.cconv(x)\n",
        "        normed = self.cbn(conved)\n",
        "        acted = self.leaky_relu(normed)\n",
        "\n",
        "        return acted"
      ],
      "metadata": {
        "id": "XgUrwf8cCr45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layer Decoder:"
      ],
      "metadata": {
        "id": "t_mwsQ_bDJsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, filter_size=(7,5), stride_size=(2,2), in_channels=1, out_channels=45,\n",
        "                 output_padding=(0,0), padding=(0,0), last_layer=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.filter_size = filter_size\n",
        "        self.stride_size = stride_size\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.output_padding = output_padding\n",
        "        self.padding = padding\n",
        "\n",
        "        self.last_layer = last_layer\n",
        "\n",
        "        self.cconvt = ComplexConvTranspose2d(in_channels=self.in_channels, out_channels=self.out_channels,\n",
        "                             kernel_size=self.filter_size, stride=self.stride_size, output_padding=self.output_padding, padding=self.padding)\n",
        "\n",
        "        self.cbn = ComplexBatchNorm2d(num_features=self.out_channels)\n",
        "\n",
        "        self.leaky_relu = nn.LeakyReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        conved = self.cconvt(x)\n",
        "\n",
        "        if not self.last_layer:\n",
        "            normed = self.cbn(conved)\n",
        "            output = self.leaky_relu(normed)\n",
        "        else:\n",
        "            m_phase = conved / (torch.abs(conved) + 1e-8)\n",
        "            m_mag = torch.tanh(torch.abs(conved))\n",
        "            output = m_phase * m_mag\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "4OMBVSdbDMBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funzione di Loss"
      ],
      "metadata": {
        "id": "I0E8Uf8UDiP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wsdr_fn(x_, y_pred_, y_true_, eps=1e-8): # Weighted SDR loss\n",
        "    # waveform nel dominio del tempo\n",
        "    y_true_ = torch.squeeze(y_true_, 1)\n",
        "    y_true = torch.istft(y_true_, n_fft=n_fft, hop_length=hop_length, normalized=True)\n",
        "    x_ = torch.squeeze(x_, 1)\n",
        "    x = torch.istft(x_, n_fft=n_fft, hop_length=hop_length, normalized=True)\n",
        "\n",
        "    y_pred = y_pred_.flatten(1)\n",
        "    y_true = y_true.flatten(1)\n",
        "    x = x.flatten(1)\n",
        "\n",
        "\n",
        "    def sdr_fn(true, pred, eps=1e-8):\n",
        "        num = torch.sum(true * pred, dim=1)\n",
        "        den = torch.norm(true, p=2, dim=1) * torch.norm(pred, p=2, dim=1)\n",
        "        return -(num / (den + eps))\n",
        "\n",
        "    # true and estimated noise\n",
        "    z_true = x - y_true\n",
        "    z_pred = x - y_pred\n",
        "\n",
        "    a = torch.sum(y_true**2, dim=1) / (torch.sum(y_true**2, dim=1) + torch.sum(z_true**2, dim=1) + eps)\n",
        "    wSDR = a * sdr_fn(y_true, y_pred) + (1 - a) * sdr_fn(z_true, z_pred)\n",
        "    return torch.mean(wSDR)\n",
        "\n",
        "def getMetricsonLoader(loader, net, use_net=True):\n",
        "    net.eval()\n",
        "    metric_names = [\"SNR\", \"SSNR\"]\n",
        "    overall_metrics = [[] for _ in range(len(metric_names))]\n",
        "\n",
        "    # itera sui batch del DataLoader\n",
        "    for i, data in enumerate(loader):\n",
        "        noisy = data[0]\n",
        "        clean = data[1]\n",
        "\n",
        "        # se vogliamo usare il modello per produrre il segnale pulito stimato\n",
        "        if use_net:\n",
        "            x_est = net(noisy.cuda(), is_istft=True)\n",
        "            x_est_np = x_est.view(-1).detach().cpu().numpy()\n",
        "        else:\n",
        "            x_est_np = torch.istft(torch.squeeze(noisy, 1), n_fft=n_fft, hop_length=hop_length, normalized=True).view(-1).detach().cpu().numpy()\n",
        "\n",
        "        x_clean_np = torch.istft(torch.squeeze(clean, 1), n_fft=n_fft, hop_length=hop_length, normalized=True).view(-1).detach().cpu().numpy()\n",
        "\n",
        "        # assumiamo che lavori a 48 kHz.\n",
        "        metrics = AudioMetricsGeneral(x_clean_np, x_est_np, 48000)\n",
        "\n",
        "        overall_metrics[0].append(metrics.SNR)\n",
        "        overall_metrics[1].append(metrics.SSNR)\n",
        "\n",
        "    print(\"\\nSample metrics computed\")\n",
        "    results = {}\n",
        "    for i in range(len(metric_names)):\n",
        "        temp = {\n",
        "            \"Mean\": np.mean(overall_metrics[i]),\n",
        "            \"STD\": np.std(overall_metrics[i]),\n",
        "            \"Min\": min(overall_metrics[i]),\n",
        "            \"Max\": max(overall_metrics[i]),\n",
        "        }\n",
        "        results[metric_names[i]] = temp\n",
        "\n",
        "    addon = \"(cleaned by model)\" if use_net else \"(pre denoising)\"\n",
        "    print(\"Metrics on test data\", addon)\n",
        "    for i in range(len(metric_names)):\n",
        "        print(\"{} : {:.3f} +/- {:.3f}\".format(metric_names[i], np.mean(overall_metrics[i]), np.std(overall_metrics[i])))\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "XLFll7XfDlR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Allenamento epoche"
      ],
      "metadata": {
        "id": "_WRdpPg7L60k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(net, train_loader, loss_fn, optimizer):\n",
        "    net.train()\n",
        "    train_ep_loss = 0. # loss totale durante l'epoca\n",
        "    counter = 0 # contatore n. di batch elaborati\n",
        "\n",
        "    for noisy_x, clean_x in train_loader: # iterazione per ogni batch\n",
        "\n",
        "        noisy_x, clean_x = noisy_x.cuda(), clean_x.cuda()\n",
        "\n",
        "        # azzeramento gradienti\n",
        "        net.zero_grad()\n",
        "\n",
        "        # output del modello per il batch corrente\n",
        "        pred_x = net(noisy_x)\n",
        "\n",
        "        # calcolo della perdita, loss\n",
        "        loss = loss_fn(noisy_x, pred_x, clean_x)\n",
        "        # backpropagation e aggiornamento dei pesi\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_ep_loss += loss.item()\n",
        "        counter += 1\n",
        "\n",
        "    # calcolo della perdita media per l'epoca\n",
        "    train_ep_loss /= counter\n",
        "\n",
        "    # clear cache\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    return train_ep_loss"
      ],
      "metadata": {
        "id": "Cq1pXZKwMKkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validazione del modello durante il training"
      ],
      "metadata": {
        "id": "OTvg0ce0PUtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_epoch(net, test_loader, loss_fn, use_net=True):\n",
        "    net.eval()\n",
        "    test_ep_loss = 0.\n",
        "    counter = 0.\n",
        "\n",
        "    # Calcolo diretto della loss su ogni batch\n",
        "    '''\n",
        "    for noisy_x, clean_x in test_loader:\n",
        "        # get the output from the model\n",
        "        noisy_x, clean_x = noisy_x.cuda(), clean_x.cuda()\n",
        "        pred_x = net(noisy_x)\n",
        "\n",
        "        # calculate loss\n",
        "        loss = loss_fn(noisy_x, pred_x, clean_x)\n",
        "        # Calc the metrics here\n",
        "        test_ep_loss += loss.item()\n",
        "\n",
        "        counter += 1\n",
        "\n",
        "    test_ep_loss /= counter\n",
        "    '''\n",
        "\n",
        "    #print(\"Actual compute done...testing now\")\n",
        "\n",
        "    #Calcolo metriche sui dati di test\n",
        "    testmet = getMetricsonLoader(test_loader,net,use_net)\n",
        "\n",
        "    # clear cache\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return test_ep_loss, testmet"
      ],
      "metadata": {
        "id": "yRXqzT3GPYil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stampa della Loss del train e del test durante l'allenamento"
      ],
      "metadata": {
        "id": "8vp2FFhaW-3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utile per capire quando e come si sta allenando bene o male la rete"
      ],
      "metadata": {
        "id": "9Wy1dz9HXX8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, train_loader, test_loader, loss_fn, optimizer, scheduler, epochs):\n",
        "\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "\n",
        "    for e in tqdm(range(epochs)): # itera sulle epoche (con barra di avanzamento visibile - tqdm)\n",
        "\n",
        "        # esegui un’epoca di training\n",
        "        train_loss = train_epoch(net, train_loader, loss_fn, optimizer)\n",
        "        test_loss = 0\n",
        "        # aggiorno lo scheduler (per ridurre il learning rate progressivamente)\n",
        "        scheduler.step()\n",
        "        print(\"Saving model....\")\n",
        "\n",
        "        # eseguo un’epoca di test (validazione)\n",
        "        with torch.no_grad():\n",
        "            test_loss, testmet = test_epoch(net, test_loader, loss_fn,use_net=True)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        test_losses.append(test_loss)\n",
        "\n",
        "        with open(\"/results.txt\",\"a\") as f:\n",
        "            f.write(\"Epoch :\"+str(e+1) + \"\\n\" + str(testmet))\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "        print(\"OPed to txt\")\n",
        "\n",
        "        # salva i modelli\n",
        "        torch.save(net.state_dict(), '/Weights/dc20_model_'+str(e+1)+'.pth')\n",
        "        torch.save(optimizer.state_dict(), '/Weights/dc20_opt_'+str(e+1)+'.pth')\n",
        "\n",
        "        print(\"Models saved\")\n",
        "\n",
        "        # clear cache\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        # per stampare a video i risultati invece che sul file .txt\n",
        "        #print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "        #              \"Loss: {:.6f}...\".format(train_loss),\n",
        "        #              \"Test Loss: {:.6f}\".format(test_loss))\n",
        "    return train_loss, test_loss"
      ],
      "metadata": {
        "id": "RraLNmxHXS4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modello a 20 layer della DCUNet"
      ],
      "metadata": {
        "id": "b5FhkO2JZ60q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DCUnet20(nn.Module):\n",
        "    \"\"\"\n",
        "    Deep Complex U-Net class of the model.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_fft=64, hop_length=16):\n",
        "        super().__init__()\n",
        "\n",
        "        # istft\n",
        "        self.n_fft = n_fft\n",
        "        self.hop_length = hop_length\n",
        "\n",
        "        self.set_size(model_complexity=int(45//1.414), input_channels=1, model_depth=20)\n",
        "\n",
        "        # costruzione degli encoder\n",
        "        self.encoders = []\n",
        "        self.model_length = 20 // 2  # → 10 encoder e 10 decoder\n",
        "\n",
        "        for i in range(self.model_length):\n",
        "            module = Encoder(in_channels=self.enc_channels[i], out_channels=self.enc_channels[i + 1],\n",
        "                             filter_size=self.enc_kernel_sizes[i], stride_size=self.enc_strides[i], padding=self.enc_paddings[i])\n",
        "            self.add_module(\"encoder{}\".format(i), module)\n",
        "            self.encoders.append(module)\n",
        "\n",
        "        # costruzione dei decoder\n",
        "        self.decoders = []\n",
        "\n",
        "        for i in range(self.model_length):\n",
        "            if i != self.model_length - 1:\n",
        "                module = Decoder(in_channels=self.dec_channels[i] + self.enc_channels[self.model_length - i], out_channels=self.dec_channels[i + 1],\n",
        "                                 filter_size=self.dec_kernel_sizes[i], stride_size=self.dec_strides[i], padding=self.dec_paddings[i],\n",
        "                                 output_padding=self.dec_output_padding[i])\n",
        "            else:\n",
        "                module = Decoder(in_channels=self.dec_channels[i] + self.enc_channels[self.model_length - i], out_channels=self.dec_channels[i + 1],\n",
        "                                 filter_size=self.dec_kernel_sizes[i], stride_size=self.dec_strides[i], padding=self.dec_paddings[i],\n",
        "                                 output_padding=self.dec_output_padding[i], last_layer=True)\n",
        "            self.add_module(\"decoder{}\".format(i), module)\n",
        "            self.decoders.append(module)\n",
        "\n",
        "\n",
        "    def forward(self, x, is_istft=True):\n",
        "\n",
        "        # salva input originale\n",
        "        # print('x : ', x.shape)\n",
        "        orig_x = x\n",
        "        xs = []\n",
        "        # passaggio attraverso gli encoder\n",
        "        for i, encoder in enumerate(self.encoders):\n",
        "            xs.append(x)\n",
        "            x = encoder(x)\n",
        "            # print('Encoder : ', x.shape)\n",
        "\n",
        "        # passaggio attraverso i decoder\n",
        "        p = x\n",
        "        for i, decoder in enumerate(self.decoders):\n",
        "            p = decoder(p)\n",
        "            if i == self.model_length - 1:\n",
        "                break\n",
        "            # print('Decoder : ', p.shape)\n",
        "            p = torch.cat([p, xs[self.model_length - 1 - i]], dim=1)\n",
        "\n",
        "        # u9 - the mask\n",
        "        # applicazione del mask\n",
        "        mask = p\n",
        "\n",
        "        # print('mask : ', mask.shape)\n",
        "\n",
        "        output = mask * orig_x\n",
        "        output = torch.squeeze(output, 1)\n",
        "\n",
        "        # ricostruzione del segnale audio - se indicato, applica la trasformata inversa STFT per tornare dal dominio spettrale al dominio tempo\n",
        "        if is_istft:\n",
        "            output = torch.istft(output, n_fft=self.n_fft, hop_length=self.hop_length, normalized=True)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "    def set_size(self, model_complexity, model_depth=20, input_channels=1):\n",
        "      # definisce tutte le dimensioni e i parametri per encoder e decoder, specifici per la versione a 20 layer\n",
        "\n",
        "        if model_depth == 20:\n",
        "            self.enc_channels = [input_channels,\n",
        "                                 model_complexity,\n",
        "                                 model_complexity,\n",
        "                                 model_complexity * 2,\n",
        "                                 model_complexity * 2,\n",
        "                                 model_complexity * 2,\n",
        "                                 model_complexity * 2,\n",
        "                                 model_complexity * 2,\n",
        "                                 model_complexity * 2,\n",
        "                                 model_complexity * 2,\n",
        "                                 128]\n",
        "\n",
        "            self.enc_kernel_sizes = [(7, 1),\n",
        "                                     (1, 7),\n",
        "                                     (6, 4),\n",
        "                                     (7, 5),\n",
        "                                     (5, 3),\n",
        "                                     (5, 3),\n",
        "                                     (5, 3),\n",
        "                                     (5, 3),\n",
        "                                     (5, 3),\n",
        "                                     (5, 3)]\n",
        "\n",
        "            self.enc_strides = [(1, 1),\n",
        "                                (1, 1),\n",
        "                                (2, 2),\n",
        "                                (2, 1),\n",
        "                                (2, 2),\n",
        "                                (2, 1),\n",
        "                                (2, 2),\n",
        "                                (2, 1),\n",
        "                                (2, 2),\n",
        "                                (2, 1)]\n",
        "\n",
        "            self.enc_paddings = [(3, 0),\n",
        "                                 (0, 3),\n",
        "                                 (0, 0),\n",
        "                                 (0, 0),\n",
        "                                 (0, 0),\n",
        "                                 (0, 0),\n",
        "                                 (0, 0),\n",
        "                                 (0, 0),\n",
        "                                 (0, 0),\n",
        "                                 (0, 0)]\n",
        "\n",
        "            self.dec_channels = [0,\n",
        "                                 model_complexity * 2,\n",
        "                                 model_complexity * 2,\n",
        "                                 model_complexity * 2,\n",
        "                                 model_complexity * 2,\n",
        "                                 model_complexity * 2,\n",
        "                                 model_complexity * 2,\n",
        "                                 model_complexity * 2,\n",
        "                                 model_complexity,\n",
        "                                 model_complexity,\n",
        "                                 1]\n",
        "\n",
        "            self.dec_kernel_sizes = [(6, 3),\n",
        "                                     (6, 3),\n",
        "                                     (6, 3),\n",
        "                                     (6, 4),\n",
        "                                     (6, 3),\n",
        "                                     (6, 4),\n",
        "                                     (8, 5),\n",
        "                                     (7, 5),\n",
        "                                     (1, 7),\n",
        "                                     (7, 1)]\n",
        "\n",
        "            self.dec_strides = [(2, 1), #\n",
        "                                (2, 2), #\n",
        "                                (2, 1), #\n",
        "                                (2, 2), #\n",
        "                                (2, 1), #\n",
        "                                (2, 2), #\n",
        "                                (2, 1), #\n",
        "                                (2, 2), #\n",
        "                                (1, 1),\n",
        "                                (1, 1)]\n",
        "\n",
        "            self.dec_paddings = [(0, 0),\n",
        "                                 (0, 0),\n",
        "                                 (0, 0),\n",
        "                                 (0, 0),\n",
        "                                 (0, 0),\n",
        "                                 (0, 0),\n",
        "                                 (0, 0),\n",
        "                                 (0, 0),\n",
        "                                 (0, 3),\n",
        "                                 (3, 0)]\n",
        "\n",
        "            self.dec_output_padding = [(0,0),\n",
        "                                       (0,0),\n",
        "                                       (0,0),\n",
        "                                       (0,0),\n",
        "                                       (0,0),\n",
        "                                       (0,0),\n",
        "                                       (0,0),\n",
        "                                       (0,0),\n",
        "                                       (0,0),\n",
        "                                       (0,0)]\n",
        "        else:\n",
        "            raise ValueError(\"Unknown model depth : {}\".format(model_depth))\n"
      ],
      "metadata": {
        "id": "NWQZYjNTaFWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Allenamento della rete"
      ],
      "metadata": {
        "id": "mUz-5hUCFaX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "dcunet20 = DCUnet20(n_fft, hop_length)\n",
        "\n",
        "optimizer = torch.optim.Adam(dcunet20.parameters())\n",
        "loss_fn = wsdr_fn  # funzione di perdita weighted SDR loss (metrica per valutare il miglioramento del segnale)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1) # scheduler abbassa il learning rate ogni epoca\n",
        "\n",
        "# per riprendere l’allenamento da un checkpoint salvato in precedenza\n",
        "# model_checkpoint = torch.load(path_to_model)\n",
        "# dcunet20.load_state_dict(model_checkpoint)\n",
        "\n",
        "# lancia l’allenamento del modello per 4 epoche, salvando le perdite di training e validation.\n",
        "train_losses, validation_losses = train(dcunet20, train_loader, test_loader, loss_fn, optimizer, scheduler, 4)"
      ],
      "metadata": {
        "id": "q7sBQ4qyFfFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Usare una rete già pre-allenata"
      ],
      "metadata": {
        "id": "9ay63BHhPQhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ad esempio con i rumori bianchi\n",
        "model_weights_path = \"Pretrained_Weights/Noise2Noise/white.pth\"\n",
        "dcunet20 = DCUnet20(n_fft, hop_length)\n",
        "optimizer = torch.optim.Adam(dcunet20.parameters())\n",
        "checkpoint = torch.load(model_weights_path)\n",
        "\n",
        "dcunet20.load_state_dict(checkpoint)\n",
        "\n",
        "# caricamento dei dati di test\n",
        "test_noisy_files = sorted(list(Path(\"Samples/Sample_Test_Input\").rglob('*.wav')))\n",
        "test_clean_files = sorted(list(Path(\"Samples/Sample_Test_Target\").rglob('*.wav')))\n",
        "\n",
        "test_dataset = Noise2NoiseDataset(test_noisy_files, test_clean_files, n_fft, hop_length)\n",
        "test_loader_single_unshuffled = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# inferenza su un singolo file, ad esempio quello a indice 4\n",
        "index = 4\n",
        "dcunet20.eval()\n",
        "test_loader_single_unshuffled_iter = iter(test_loader_single_unshuffled)\n",
        "\n",
        "x_n, x_c = next(test_loader_single_unshuffled_iter)\n",
        "for _ in range(index):\n",
        "    x_n, x_c = next(test_loader_single_unshuffled_iter)\n",
        "\n",
        "# esegue la predizione del modello: prende in input lo spettrogramma del file rumoroso e stima l’audio pulito in dominio temporale (is_istft=True fa la trasformata inversa per tornare al segnale audio)\n",
        "x_est = dcunet20(x_n, is_istft=True)\n",
        "\n",
        "# converte il risultato del modello in un array NumPy audio (mono)\n",
        "x_est_np = x_est[0].view(-1).detach().cpu().numpy()\n",
        "\n",
        "# convertono anche gli audio pulito (x_c) e rumoroso (x_n) dal dominio della frequenza al dominio del tempo per il confronto\n",
        "x_c_np = torch.istft(torch.squeeze(x_c[0], 1), ...)\n",
        "x_n_np = torch.istft(torch.squeeze(x_n[0], 1), ...)\n",
        "\n",
        "#valutazione con le metriche\n",
        "metrics = AudioMetricsGeneral(x_c_np, x_est_np, SAMPLE_RATE)\n",
        "print(metrics.display())\n",
        "\n",
        "#visualizzazione della forma d'onda del segnale rumoroso\n",
        "plt.plot(x_n_np)\n",
        "plt.title(\"Noisy signal\")\n",
        "plt.show()\n",
        "\n",
        "#visualizzazione della forma d'onda del segnale pulito\n",
        "plt.plot(x_c_np)\n",
        "plt.title(\"Clean signal\")\n",
        "plt.show()\n",
        "\n",
        "# salvataggio del file audio\n",
        "# se non è normalizzato, potresti volerlo normalizzare per evitare il clipping\n",
        "x_est_np = x_est_np / np.max(np.abs(x_est_np))  # Normalizza tra -1 e 1 (opzionale, ma consigliato)\n",
        "\n",
        "# 2. Ridimensiona l'array per avere forma (1, N) → 1 canale, N campioni\n",
        "x_est_np = np.reshape(x_est_np, (1, -1))\n",
        "\n",
        "# 3. Converti l'array NumPy in tensor PyTorch\n",
        "x_est_tensor = torch.from_numpy(x_est_np).float()\n",
        "\n",
        "# 4. Specifica il percorso di output\n",
        "output_path = Path(\"Samples/denoised.wav\")\n",
        "\n",
        "# 5. Specifica il sample rate\n",
        "sample_rate = SAMPLE_RATE  # ad esempio: 16000 o 48000\n",
        "\n",
        "# 6. Salva l'audio in formato WAV a 16 bit\n",
        "torchaudio.save(\n",
        "    filepath=str(output_path),         # percorso del file .wav\n",
        "    src=x_est_tensor,                  # tensor PyTorch (1, num_samples)\n",
        "    sample_rate=sample_rate,           # frequenza di campionamento\n",
        "    bits_per_sample=16                 # precisione a 16 bit\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "IN_RTn-kPTzm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}